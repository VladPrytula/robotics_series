{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Goal-Conditioned Robotic Manipulation","text":"<p>A Docker-first reinforcement learning laboratory for studying sparse-reward manipulation tasks.</p>"},{"location":"#what-is-this","title":"What Is This?","text":"<p>This repository teaches you to train neural network policies that control a simulated Fetch robot arm to manipulate objects to arbitrary goal positions. The approach uses SAC + HER (Soft Actor-Critic with Hindsight Experience Replay) to handle sparse binary rewards.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Clone and enter the repository\ngit clone https://github.com/VladPrytula/robotics_series.git\ncd robotics_series\n\n# Run proof-of-life (verifies GPU, MuJoCo, rendering, training)\nbash docker/dev.sh python scripts/ch00_proof_of_life.py all\n</code></pre>"},{"location":"#the-problem-we-solve","title":"The Problem We Solve","text":"<p>You want a robot that can reach any position, push objects to any location, pick and place any object--with goals specified at runtime, not programming time.</p> <p>This is hard because:</p> <ul> <li>Sparse rewards: Binary success/failure feedback (no partial credit)</li> <li>Continuous control: 4D velocity commands at 25Hz</li> <li>Goal generalization: Infinite possible goals, cannot train separately for each</li> </ul>"},{"location":"#the-method","title":"The Method","text":"<p>The algorithm choice follows from problem constraints:</p> Constraint Implication Solution Continuous actions Need policy gradient Actor-critic Sparse rewards Need sample reuse Off-policy (replay buffer) Goal conditioning Need goal relabeling HER Exploration Need stochastic policy SAC (max entropy) <p>Result: SAC + HER is the methodologically appropriate solution.</p>"},{"location":"#curriculum","title":"Curriculum","text":"Week Topic Done When 0 Proof of Life <code>ppo_smoke.zip</code> exists 1 Environment Anatomy Reward check passes 2-10 Training, HER, Robustness See syllabus"},{"location":"#math-rendering-test","title":"Math Rendering Test","text":"<p>Inline math: \\(\\pi^* = \\arg\\max_\\pi J(\\pi)\\)</p> <p>Block math:</p> \\[ J(\\pi) = \\mathbb{E}_{g \\sim p(g)} \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{T} \\gamma^t R(s_t, a_t, s_{t+1}, g) \\right] \\] <p>Goal-conditioned MDP: \\(\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{G}, P, R, \\phi, \\gamma)\\)</p>"},{"location":"syllabus/","title":"Spark DGX Robotics RL Syllabus (Executable, 10 weeks)","text":"<p>This is a step-by-step plan you can execute on Spark DGX to learn goal-conditioned manipulation RL and build a reproducible training/evaluation workflow around Gymnasium-Robotics Fetch tasks.</p> <p>If you prefer chapter-style \u201ctextbook\u201d notes, start with: - <code>tutorials/ch00_containerized_dgx_proof_of_life.md</code> - <code>tutorials/ch01_fetch_env_anatomy.md</code></p>"},{"location":"syllabus/#core-references-keep-open","title":"Core references (keep open)","text":"<ul> <li>Sutton &amp; Barto (concepts)</li> <li>Spinning Up PPO/SAC (debugging + failure modes)</li> <li>Modern Robotics (kinematics intuition + baselines)</li> </ul>"},{"location":"syllabus/#tooling-stack-standardize-early","title":"Tooling stack (standardize early)","text":"<ul> <li>Gymnasium-Robotics Fetch suite (goal-conditioned dict observations + success metrics) (Fetch docs)</li> <li>MuJoCo (maintained <code>mujoco</code> Python bindings; no <code>mujoco-py</code>) (MuJoCo)</li> <li>Stable-Baselines3 (PPO/SAC/TD3 + <code>HerReplayBuffer</code> for sparse goals; HER is off-policy only) (SB3 HER)</li> <li>Tracking: W&amp;B or TensorBoard</li> </ul>"},{"location":"syllabus/#non-negotiables-fetch-shape","title":"Non-negotiables (Fetch \"shape\")","text":"<ul> <li>Observations are dicts: <code>observation</code>, <code>desired_goal</code>, <code>achieved_goal</code>. This is what makes HER/relabeling natural. (Fetch docs)</li> <li>Actions are 4D: small Cartesian deltas <code>dx, dy, dz</code> + gripper open/close. It is not torque control. (Fetch docs)</li> </ul>"},{"location":"syllabus/#why-sac-her-deriving-the-method-from-the-problem","title":"Why SAC + HER? (Deriving the method from the problem)","text":"<p>The algorithm choice is not arbitrary. It follows from the problem constraints:</p> <p>1. Continuous actions \u2192 Actor-Critic. The action space is \\(\\mathbb{R}^4\\). Pure value-based methods (DQN) require \\(\\arg\\max_a Q(s,a)\\), which is intractable for continuous \\(a\\) without expensive discretization. We need a policy network that outputs actions directly, plus a critic to reduce variance. This means actor-critic.</p> <p>2. Sparse rewards \u2192 Off-Policy. In Gymnasium-Robotics, sparse step rewards are 0 when the success threshold is met and \u22121 otherwise (equivalently, a binary success indicator 1/0 up to a constant shift). Either way, informative signal is rare. On-policy methods (PPO) discard data after each update\u2014wasteful when positive signal is scarce. Off-policy methods (SAC, TD3) store transitions in a replay buffer and reuse them. This is essential for sample efficiency with sparse rewards.</p> <p>3. Goal-conditioned + sparse \u2192 HER. A trajectory that fails to reach goal \\(g\\) still demonstrates how to reach whatever state \\(s_T\\) it ended in. Hindsight Experience Replay relabels transitions: replace \\(g\\) with the achieved goal \\(g' = g_{\\text{achieved}}(s_T)\\), recompute rewards, and store both versions. Failed attempts become successful demonstrations for different goals. This manufactures dense signal from sparse feedback.</p> <p>4. Large goal space \u2192 Maximum Entropy (SAC). HER needs diverse experience to relabel. SAC's entropy bonus encourages exploration without explicit schedules: $\\(\\pi^* = \\arg\\max_\\pi \\mathbb{E}\\left[\\sum_t \\gamma^t (R_t + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t)))\\right]\\)$ The temperature \\(\\alpha\\) (often auto-tuned) balances exploration and exploitation.</p> <p>Summary: | Constraint | Requirement | Solution | |------------|-------------|----------| | Continuous actions | Direct policy output | Actor-critic | | Sparse rewards | Sample reuse | Off-policy (replay buffer) | | Goal-conditioned sparse | Learn from failures | HER | | Large goal space | Exploration | Entropy bonus (SAC) |</p> <p>The method is SAC + HER. This is derived, not chosen from a menu.</p> <p>Where does PPO fit? PPO is on-policy and cannot use HER. It works for dense rewards (where every step provides signal) and serves as a sanity-check baseline in Week 2. But for the core sparse-reward tasks, SAC + HER is required.</p>"},{"location":"syllabus/#reproducibility-target-realistic-on-dgx","title":"Reproducibility target (realistic on DGX)","text":"<p>You are not aiming for bitwise-identical curves. You are aiming for: 1) same code + same seed =&gt; evaluation metrics within a tolerance on the same machine class; and 2) across 3\u20135 seeds =&gt; mean\u00b1std is stable enough for ablations.</p>"},{"location":"syllabus/#spark-dgx-checklist-do-this-once","title":"Spark DGX checklist (do this once)","text":"<ul> <li>Prefer running everything in a GPU-enabled container (<code>docker run --gpus all</code>) so you don\u2019t install/pin Python packages on the DGX host.</li> <li>If the host does not provide <code>python</code>, do not fight it: run all Python via <code>bash docker/dev.sh python ...</code>.</li> <li>Run long jobs in <code>tmux</code>/<code>screen</code> or your scheduler (avoid \u201cSSH session died\u201d losses).</li> <li>Prefer a fast filesystem for checkpoints/replay/results if available.</li> <li>Headless rendering: try <code>MUJOCO_GL=egl</code> first; fall back to <code>osmesa</code>/<code>xvfb-run</code> if EGL isn\u2019t available.</li> </ul>"},{"location":"syllabus/#keep-runs-alive-tmux","title":"Keep runs alive (tmux)","text":"<pre><code>tmux new -s rl\n# detach: Ctrl-b d\n# reattach:\ntmux attach -t rl\n</code></pre>"},{"location":"syllabus/#quickstart-3060-min-dgx-proof-of-life","title":"Quickstart (30\u201360 min): DGX proof-of-life","text":"<p>Goal: \u201cFetch env loads, can render headless, PPO can train for 50k steps\u201d.</p>"},{"location":"syllabus/#0-preflight-host","title":"0) Preflight (host)","text":"<pre><code>cd /home/vladp/src/robotics\nhostname\nnvidia-smi | head\ndocker --version\n\n# Minimal GPU-in-container check (pick any CUDA image you have access to):\ndocker run --rm --gpus all nvcr.io/nvidia/pytorch:25.12-py3 nvidia-smi\n# Another local option (CUDA base image):\ndocker run --rm --gpus all nvcr.io/nvidia/cuda:13.0.1-devel-ubuntu24.04 nvidia-smi\n# Docker Hub example (substitute a tag you have/pulled):\ndocker run --rm --gpus all nvidia/cuda:&lt;tag&gt; nvidia-smi\n</code></pre>"},{"location":"syllabus/#1-one-command-proof-of-life-recommended","title":"1) One-command proof-of-life (recommended)","text":"<p>This runs everything from the host (no interactive shell required): <pre><code>cd /home/vladp/src/robotics\nbash docker/dev.sh python scripts/ch00_proof_of_life.py all\n</code></pre></p> <p>Done when: <code>smoke_frame.png</code> exists and <code>ppo_smoke.zip</code> is created without crashes.</p>"},{"location":"syllabus/#2-optional-start-an-interactive-docker-dev-shell","title":"2) (Optional) Start an interactive Docker dev shell","text":"<p>Recommended (Python + CUDA already included): <pre><code>cd /home/vladp/src/robotics\ndocker run --rm -it --gpus all --ipc=host \\\n  -e MUJOCO_GL=egl \\\n  -e PYOPENGL_PLATFORM=egl \\\n  -v \"$PWD:/workspace\" -w /workspace \\\n  nvcr.io/nvidia/pytorch:25.12-py3 bash\n</code></pre> If this creates root-owned files in your repo, use the wrapper below (runs as your user).</p> <p>Convenience wrapper (same thing, but shorter to type): <pre><code>cd /home/vladp/src/robotics\nbash docker/run.sh\n</code></pre></p> <p>Auto-setup wrapper (recommended): creates <code>./.venv</code>, installs <code>requirements.txt</code>, and activates the venv. <pre><code>cd /home/vladp/src/robotics\nbash docker/dev.sh\n</code></pre> Edit <code>requirements.txt</code> to add/remove Python deps; <code>docker/dev.sh</code> will reinstall when it changes. If <code>robotics-rl:latest</code> is not present, <code>docker/dev.sh</code> builds it once (MuJoCo EGL/OSMesa system deps).</p> <p>Optional: build a pinned \u201cproject image\u201d (useful once things work and you want reproducibility): <pre><code>cd /home/vladp/src/robotics\nbash docker/build.sh robotics-rl:latest\nIMAGE=robotics-rl:latest bash docker/run.sh\n</code></pre></p>"},{"location":"syllabus/#3-fallback-host-venv-only-if-docker-isnt-possible","title":"3) Fallback: host venv (only if Docker isn\u2019t possible)","text":"<pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npython -m pip install -U pip wheel\n</code></pre> <p>Install packages (pin exact versions after this works) <pre><code>python -m pip install \"gymnasium&gt;=0.29\" \"gymnasium-robotics&gt;=1.2\" \"mujoco&gt;=3\" \"stable-baselines3&gt;=2.2\" tensorboard imageio\n</code></pre> Optional: <pre><code>python -m pip install wandb\n</code></pre></p>"},{"location":"syllabus/#project-contract-set-up-in-week-0-dont-churn-it-later","title":"Project contract (set up in Week 0; don\u2019t churn it later)","text":""},{"location":"syllabus/#folder-skeleton-minimum","title":"Folder skeleton (minimum)","text":"<p>Create these once and keep the rest of your work inside this structure: <pre><code>mkdir -p configs envs wrappers callbacks scripts analysis results checkpoints videos\n</code></pre></p>"},{"location":"syllabus/#stable-cli-contract","title":"Stable CLI contract","text":"<p>Your future self depends on this being stable.</p> <p><code>train.py</code> - <code>--algo {ppo,sac,td3}</code> - <code>--env {FetchReachDense-v4, FetchReach-v4, FetchPush-v4, FetchPickAndPlace-v4, ...}</code> - <code>--seed</code> - <code>--device {auto,cpu,cuda}</code> - <code>--n-envs</code> - <code>--total-steps</code> - <code>--track {tb,none}</code></p> <p><code>eval.py</code> - <code>--ckpt path</code> - <code>--env ...</code> - <code>--n-episodes</code> - <code>--seeds list_or_range</code> (or a base <code>--seed</code>) - <code>--deterministic</code> (flag) - <code>--json-out results/metrics.json</code> - <code>--video</code> (flag)</p>"},{"location":"syllabus/#metadata-you-always-log","title":"Metadata you always log","text":"<p>Commit hash, Python/pip lock, <code>mujoco</code>/<code>gymnasium-robotics</code>/SB3 versions, env id, seed(s), device, <code>n_envs</code>, wallclock, steps/sec, and evaluation protocol. This repo\u2019s default mechanism: - <code>train.py</code> writes <code>&lt;checkpoint&gt;.meta.json</code> next to the <code>.zip</code>. - <code>eval.py</code> writes a JSON report that includes versions + per-episode metrics. - <code>scripts/print_versions.py</code> writes an environment snapshot for the machine/container.</p>"},{"location":"syllabus/#metrics-you-track-from-week-1-onward-dont-change-later","title":"Metrics you track from Week 1 onward (don\u2019t change later)","text":"<p>Per-episode: - success (env-provided if present, plus your own goal-distance threshold) - final goal distance - return - episode length - time-to-success (first timestep that meets success) - action smoothness proxy <code>\u03a3 ||a_t - a_{t-1}||^2</code> - max <code>|a|</code></p> <p>Aggregate: - success-rate mean\u00b1CI (binomial CI is fine) - return mean\u00b1std - robustness curves (success vs noise/randomization)</p>"},{"location":"syllabus/#week-by-week-plan","title":"Week-by-week plan","text":"<p>Each week has: (1) goal, (2) steps you can run, (3) \u201cdone when\u201d.</p>"},{"location":"syllabus/#week-0-dgx-setup-workflow-you-can-trust","title":"Week 0 \u2014 DGX setup + workflow you can trust","text":"<p>Goal: end-to-end train/eval works; headless rendering is solved; you can reproduce an eval JSON within tolerance.</p> <p>Steps - [ ] Snapshot versions (inside container):   - <code>bash docker/dev.sh python scripts/print_versions.py --json-out results/versions.json</code> - [ ] Validate end-to-end (GPU + Fetch + render + PPO smoke):   - <code>bash docker/dev.sh python scripts/ch00_proof_of_life.py all</code> - [ ] Repro sanity (train twice, eval both, compare within tolerance):   - <code>bash docker/dev.sh python scripts/check_repro.py --algo ppo --env FetchReachDense-v4 --total-steps 50000 --n-eval-episodes 50</code> - [ ] Decide how you\u2019ll run on Spark DGX (one of):   - <code>tmux</code>-based workflow (simple, interactive)   - scheduler job script (better for long runs; adapt to your cluster)</p> <p>Done when - Same commit runs on Spark DGX reliably. - Save/load works and <code>eval.py</code> emits a stable JSON schema.</p>"},{"location":"syllabus/#week-1-goal-conditioned-rl-literacy-dict-obs-reward-success","title":"Week 1 \u2014 Goal-conditioned RL literacy (dict obs, reward, success)","text":"<p>Goal: you understand exactly what SB3 \u201csees\u201d and how the environment computes reward/success.</p> <p>Steps - [ ] Describe obs/action schema (JSON artifact):   - <code>bash docker/dev.sh python scripts/ch01_env_anatomy.py describe --json-out results/ch01_env_describe.json</code> - [ ] Reward semantics consistency check (<code>compute_reward</code> vs env reward):   - <code>bash docker/dev.sh python scripts/ch01_env_anatomy.py reward-check --n-steps 500</code> - [ ] Random-rollout metrics (baseline sanity + metric schema preview):   - <code>bash docker/dev.sh python scripts/ch01_env_anatomy.py random-episodes --n-episodes 10 --json-out results/ch01_random_metrics.json</code> - [ ] Decide normalization:   - start with \u201cnone\u201d until Week 2 baseline is stable; add normalization once you can detect regressions via <code>eval.py</code>.</p> <p>Done when - You can plot success-rate vs steps and distributions of final distance. - Reward recomputation matches environment reward semantics.</p>"},{"location":"syllabus/#week-2-ppo-on-dense-reach-pipeline-truth-serum","title":"Week 2 \u2014 PPO on dense Reach (pipeline \u201ctruth serum\u201d)","text":"<p>Goal: a stable baseline that proves your training loop is correct.</p> <p>Steps - [ ] Train PPO on a dense Reach env:   - <code>bash docker/dev.sh python train.py --algo ppo --env FetchReachDense-v4 --seed 0 --n-envs 8 --total-steps 1000000 --out checkpoints/ppo_reachdense_v4_seed0</code> - [ ] Lock an evaluation protocol:   - separate eval env   - fixed eval seeds   - deterministic policy at eval   - N=100 episodes (or N=50 for quick iteration)   - <code>bash docker/dev.sh python eval.py --ckpt checkpoints/ppo_reachdense_v4_seed0.zip --env FetchReachDense-v4 --n-episodes 100 --seeds 0-99 --deterministic --json-out results/ppo_reachdense_v4_seed0_eval.json</code> - [ ] Add an \u201cearly warning\u201d dashboard:   - value loss explosion, entropy collapse, KL drift, success flatline</p> <p>Done when - PPO reaches consistently high success on dense Reach. - You can explain (practically) what PPO clipping is protecting you from.</p>"},{"location":"syllabus/#week-3-sac-on-dense-reach-replay-diagnostics","title":"Week 3 \u2014 SAC on dense Reach + replay diagnostics","text":"<p>Goal: your off-policy stack is correct before you add HER.</p> <p>Steps - [ ] Train SAC on dense Reach using dict obs (<code>MultiInputPolicy</code>). (SB3 examples)   - <code>bash docker/dev.sh python train.py --algo sac --env FetchReachDense-v4 --seed 0 --n-envs 8 --total-steps 1000000 --out checkpoints/sac_reachdense_v4_seed0</code>   - <code>bash docker/dev.sh python eval.py --ckpt checkpoints/sac_reachdense_v4_seed0.zip --env FetchReachDense-v4 --n-episodes 100 --seeds 0-99 --deterministic --json-out results/sac_reachdense_v4_seed0_eval.json</code> - [ ] Add replay diagnostics:   - reward histogram   - achieved vs desired goal distance distribution   - Q-values + entropy coefficient trends - [ ] Throughput scaling on DGX:   - sweep <code>n_envs</code> upward and log steps/sec</p> <p>Done when - SAC trains stably (no divergence; returns/success improve). - You can explain SAC\u2019s entropy objective (maximum-entropy RL) at a practical level. (SB3 SAC)</p>"},{"location":"syllabus/#week-4-sparse-reach-push-introduce-her-where-it-matters","title":"Week 4 \u2014 Sparse Reach + Push: introduce HER where it matters","text":"<p>Goal: demonstrate that HER is the difference-maker on sparse goals.</p> <p>Steps - [ ] Train SAC on sparse Reach (<code>FetchReach-*</code>) without HER (establish baseline difficulty). - [ ] Train SAC + <code>HerReplayBuffer</code> on sparse Reach. (SB3 HER)   - no-HER:     - <code>bash docker/dev.sh python train.py --algo sac --env FetchReach-v4 --seed 0 --n-envs 8 --total-steps 1000000 --out checkpoints/sac_reach_v4_seed0</code>   - HER:     - <code>bash docker/dev.sh python train.py --algo sac --her --her-goal-selection-strategy future --her-n-sampled-goal 4 --env FetchReach-v4 --seed 0 --n-envs 8 --total-steps 1000000 --out checkpoints/sac_her_reach_v4_seed0</code> - [ ] Repeat on sparse Push (<code>FetchPush-*</code>):   - no-HER baseline   - HER with <code>goal_selection_strategy=\"future\"</code> and <code>n_sampled_goal \u2208 {2,4,8}</code></p> <p>Done when - Clear separation in success-rate between HER and no-HER. - You can report mean\u00b1CI across 3\u20135 seeds.</p>"},{"location":"syllabus/#week-5-pickandplace-dense-debug-sparseher-curriculum","title":"Week 5 \u2014 PickAndPlace: dense debug \u2192 sparse+HER \u2192 curriculum","text":"<p>Goal: validate the pipeline on contacts/grasping without fooling yourself via accidental shaping.</p> <p>Steps - [ ] Dense validation first (if a dense PickAndPlace variant exists in your install).   - If not: use temporary shaping only for debugging, and document it clearly. - [ ] Sparse + HER:   - start from stable Push hyperparams   - increase training horizon (PickAndPlace takes longer) - [ ] Curriculum (explicit schedule):   - narrow goal range / easier initial object positions first   - gradually expand to the full distribution - [ ] Eval split:   - \u201ctrain-like\u201d distribution   - \u201cstress\u201d distribution (wider initial placements)</p> <p>Done when - Dense mode: high success + smooth behavior (no frantic oscillations). - Sparse+HER: meaningful upward trend; stress test is worse but improving.</p>"},{"location":"syllabus/#week-6-action-interface-engineering-policies-as-controllers","title":"Week 6 \u2014 Action-interface engineering (\u201cpolicies as controllers\u201d)","text":"<p>Goal: treat learned policies like controllers and quantify stability.</p> <p>Steps - [ ] Action scaling study: multiply actions by <code>{0.5, 1.0, 2.0}</code>; measure success + smoothness + time-to-success. - [ ] Add an action wrapper:   - clipping (already bounded) + optional low-pass filter   - compare oscillations/contact stability - [ ] Classical baseline (Reach + maybe Push):   - simple \u201cgo-to-goal\u201d PD-like rule in Cartesian delta space - [ ] Define a controller-centric metric bundle:   - success, time-to-success, smoothness, peak <code>|a|</code>, path-length proxy</p> <p>Done when - You can compare RL vs baseline on controller metrics (not just return). - You can separate \u201cplanning\u201d issues from \u201ccontrol\u201d issues with evidence.</p>"},{"location":"syllabus/#week-7-robustness-noise-randomization-brittleness-curves","title":"Week 7 \u2014 Robustness: noise, randomization, brittleness curves","text":"<p>Goal: quantify brittleness and improvements (don\u2019t rely on videos).</p> <p>Steps - [ ] Controlled noise injection sweeps:   - observation noise <code>\u03c3</code>   - action execution noise <code>\u03c3</code> - [ ] Domain randomization (start light):   - if physical params are hard to reach, begin with obs/action noise - [ ] Plot degradation curves with confidence bands across seeds.</p> <p>Done when - At least one robustness technique measurably helps. - You can quantify \u201chow brittle\u201d with plots.</p>"},{"location":"syllabus/#week-8-second-suite-adapter-robosuite-or-meta-world","title":"Week 8 \u2014 Second suite adapter (robosuite OR Meta-World)","text":"<p>Goal: avoid overfitting intuition to Fetch.</p> <p>Steps (pick one suite) - [ ] Implement an adapter to the same train/eval contract. - [ ] Solve 1\u20132 simple tasks. - [ ] If using robosuite, compare controller modes (this is where \u201ctorque vs OSC vs position\u201d becomes real).</p> <p>Done when - You can train/eval in a second ecosystem with minimal harness changes.</p>"},{"location":"syllabus/#week-9-engineering-grade-rl-sweeps-ablations-seed-discipline","title":"Week 9 \u2014 Engineering-grade RL: sweeps, ablations, seed discipline","text":"<p>Goal: turn tinkering into evidence.</p> <p>Steps - [ ] Seed discipline: 5 seeds per config; report mean\u00b1std + CI for success. - [ ] Minimum ablation grid:   - normalization method A vs B   - HER <code>n_sampled_goal \u2208 {2,4,8}</code>   - SAC entropy: auto vs fixed target entropy - [ ] Add a launcher:   - YAML grid spec   - Spark DGX job launcher script (scheduler or local parallel runner)   - auto-collect results into a single report</p> <p>Done when - You can answer \u201cwhat mattered?\u201d with plots + statistics. - A run is reproducible from scratch given commit+lock+config.</p>"},{"location":"syllabus/#week-10-capstone-sparse-pickandplace-robustness-targets","title":"Week 10 \u2014 Capstone: sparse PickAndPlace + robustness targets","text":"<p>Goal: one deliverable-grade model + eval protocol + write-up.</p> <p>Capstone spec - Environment: sparse PickAndPlace (<code>FetchPickAndPlace-*</code>) + HER - Training: Spark DGX, vectorized envs, periodic checkpoints - Evaluation:   - standard test distribution   - stress test (wider init positions + mild noise/randomization)   - report success-rate, time-to-success, smoothness metrics</p> <p>Deliverables - model checkpoint(s) + normalization stats - <code>eval.py</code> outputs one JSON with metrics + provenance (commit hash, versions, seeds, episode count) - short experiment card:   - observation handling + normalization   - action filtering/scaling   - hyperparams that mattered (from ablations)   - what failed and how you debugged it</p> <p>Stretch (only after capstone is stable) - Vision (RGB) will spike sample complexity - Demonstrations + BC warm-start can help manipulation - CS285 robotics-adjacent assignments as structured extensions</p>"},{"location":"reference/cli/","title":"CLI Reference","text":""},{"location":"reference/cli/#training-trainpy","title":"Training: <code>train.py</code>","text":"<p>Train RL agents on Fetch environments.</p> <pre><code>bash docker/dev.sh python train.py --algo sac --her --env FetchReach-v4 --seed 0\n</code></pre>"},{"location":"reference/cli/#arguments","title":"Arguments","text":"Argument Type Default Description <code>--algo</code> <code>{ppo,sac,td3}</code> required Algorithm <code>--env</code> string required Gymnasium env ID <code>--seed</code> int 0 Random seed <code>--device</code> <code>{auto,cpu,cuda}</code> auto Compute device <code>--n-envs</code> int 8 Parallel environments <code>--total-steps</code> int 1000000 Training timesteps <code>--her</code> flag false Enable HER (SAC/TD3 only) <code>--track</code> <code>{none,tb}</code> tb Logging backend"},{"location":"reference/cli/#ppo-specific","title":"PPO-Specific","text":"Argument Default Description <code>--ppo-n-steps</code> 1024 Steps per rollout <code>--ppo-batch-size</code> 256 Minibatch size"},{"location":"reference/cli/#sactd3-specific","title":"SAC/TD3-Specific","text":"Argument Default Description <code>--off-batch-size</code> 256 Batch size <code>--off-buffer-size</code> 1000000 Replay buffer size <code>--off-learning-starts</code> 10000 Steps before learning"},{"location":"reference/cli/#her-specific","title":"HER-Specific","text":"Argument Default Description <code>--her-n-sampled-goal</code> 4 Relabeled goals per transition <code>--her-goal-selection-strategy</code> future <code>{future,final,episode}</code>"},{"location":"reference/cli/#evaluation-evalpy","title":"Evaluation: <code>eval.py</code>","text":"<p>Evaluate trained checkpoints.</p> <pre><code>bash docker/dev.sh python eval.py --ckpt checkpoints/model.zip --env FetchReach-v4 --n-episodes 100\n</code></pre>"},{"location":"reference/cli/#arguments_1","title":"Arguments","text":"Argument Type Default Description <code>--ckpt</code> path required Checkpoint file <code>--env</code> string required Gymnasium env ID <code>--n-episodes</code> int 100 Evaluation episodes <code>--seeds</code> string \"0\" Seeds (e.g., \"0-9\" or \"0,1,2\") <code>--deterministic</code> flag false Deterministic actions <code>--json-out</code> path none Output JSON path <code>--video</code> flag false Record videos"},{"location":"reference/cli/#chapter-scripts","title":"Chapter Scripts","text":""},{"location":"reference/cli/#scriptsch00_proof_of_lifepy","title":"<code>scripts/ch00_proof_of_life.py</code>","text":"<p>Verify environment functionality.</p> <pre><code># Run all tests\nbash docker/dev.sh python scripts/ch00_proof_of_life.py all\n\n# Individual tests\nbash docker/dev.sh python scripts/ch00_proof_of_life.py list-envs\nbash docker/dev.sh python scripts/ch00_proof_of_life.py render\nbash docker/dev.sh python scripts/ch00_proof_of_life.py ppo-smoke\n</code></pre>"},{"location":"reference/cli/#scriptsch01_env_anatomypy","title":"<code>scripts/ch01_env_anatomy.py</code>","text":"<p>Inspect Fetch environment structure.</p> <pre><code># List available environments\nbash docker/dev.sh python scripts/ch01_env_anatomy.py list-envs\n\n# Describe observation/action spaces\nbash docker/dev.sh python scripts/ch01_env_anatomy.py describe --json-out results/env.json\n\n# Verify reward consistency\nbash docker/dev.sh python scripts/ch01_env_anatomy.py reward-check\n\n# Random baseline\nbash docker/dev.sh python scripts/ch01_env_anatomy.py random-episodes --n-episodes 10\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>A systematic course in goal-conditioned reinforcement learning for robotic manipulation.</p>"},{"location":"tutorials/#quick-navigation","title":"Quick Navigation","text":"Chapter Topic Key Deliverable Chapter 0 Proof of Life Working Docker environment, <code>ppo_smoke.zip</code> Chapter 1 Environment Anatomy <code>reward-check</code> passes, baseline metrics Chapter 2 PPO Baseline &gt;90% success on dense Reach, pipeline validated Chapter 3 SAC + Replay Diagnostics SAC matches PPO, Q-values stable Chapter 4 HER for Sparse Coming soon"},{"location":"tutorials/#the-learning-path","title":"The Learning Path","text":"<pre><code>Chapter 0: Can I run code?\n    |\nChapter 1: What does the robot see/do?\n    |\nChapter 2: Can I train anything? (PPO baseline, on-policy)\n    |\nChapter 3: Can I train off-policy? (SAC, replay buffer)\n    |\nChapter 4: Can I handle sparse rewards? (HER)\n    |\nChapters 5-10: Advanced topics\n</code></pre>"},{"location":"tutorials/#running-commands","title":"Running Commands","text":"<p>All commands run through Docker via the <code>docker/dev.sh</code> wrapper:</p> <pre><code># Pattern: bash docker/dev.sh &lt;command&gt;\nbash docker/dev.sh python scripts/ch03_sac_dense_reach.py all\n</code></pre>"},{"location":"tutorials/#chapter-scripts","title":"Chapter Scripts","text":"<p>Each chapter has a self-contained orchestration script:</p> <pre><code># Full pipeline (train + eval + compare)\nbash docker/dev.sh python scripts/ch00_proof_of_life.py all\nbash docker/dev.sh python scripts/ch02_ppo_dense_reach.py all --seed 0\nbash docker/dev.sh python scripts/ch03_sac_dense_reach.py all --seed 0\n\n# Partial execution\nbash docker/dev.sh python scripts/ch03_sac_dense_reach.py train --total-steps 100000\nbash docker/dev.sh python scripts/ch03_sac_dense_reach.py eval\nbash docker/dev.sh python scripts/ch03_sac_dense_reach.py compare\n</code></pre>"},{"location":"tutorials/#monitoring-training","title":"Monitoring Training","text":"<pre><code># Start TensorBoard (http://localhost:6006)\nbash docker/dev.sh tensorboard --logdir runs --bind_all\n</code></pre>"},{"location":"tutorials/#long-jobs-with-tmux","title":"Long Jobs with tmux","text":"<pre><code>tmux new -s rl                    # Start session\n# ... run training ...\n# Ctrl-b d                        # Detach\ntmux attach -t rl                 # Reattach later\n</code></pre>"},{"location":"tutorials/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"tutorials/#chapter-0-proof-of-life","title":"Chapter 0: Proof of Life","text":"<p>Goal: Verify your experimental environment works.</p> <p>You will:</p> <ul> <li>Build and enter a Docker container with GPU access</li> <li>Verify MuJoCo physics simulation runs</li> <li>Confirm headless rendering produces images</li> <li>Run a short PPO training loop</li> </ul> <p>Done when: <code>ppo_smoke.zip</code> exists and loads successfully.</p>"},{"location":"tutorials/#chapter-1-environment-anatomy","title":"Chapter 1: Environment Anatomy","text":"<p>Goal: Understand exactly what the Fetch environments provide.</p> <p>You will:</p> <ul> <li>Inspect observation space (dict with <code>observation</code>, <code>achieved_goal</code>, <code>desired_goal</code>)</li> <li>Understand action semantics (4D Cartesian velocity)</li> <li>Verify reward consistency with <code>compute_reward()</code></li> <li>Collect random baseline metrics</li> </ul> <p>Done when: <code>reward-check</code> passes and you can answer:</p> <ul> <li>What is the observation dimension for FetchReach? (10)</li> <li>What does action index 3 control? (gripper)</li> <li>Why can HER relabel goals? (<code>compute_reward</code> accepts arbitrary goals)</li> </ul>"},{"location":"tutorials/#chapter-2-ppo-on-dense-reach","title":"Chapter 2: PPO on Dense Reach","text":"<p>Goal: Validate the training pipeline with the simplest method that should work.</p> <p>You will:</p> <ul> <li>Train PPO on FetchReachDense-v4 (continuous reward signal)</li> <li>Understand PPO's clipped surrogate objective and why it exists</li> <li>Learn to read TensorBoard diagnostics for training health</li> <li>Establish baseline metrics for comparison</li> </ul> <p>Done when: Success rate &gt; 90% and you can answer:</p> <ul> <li>Why does PPO clip probability ratios instead of parameter updates?</li> <li>What does <code>clip_fraction = 0.15</code> mean in TensorBoard?</li> <li>Why use dense rewards for pipeline validation?</li> </ul>"},{"location":"tutorials/#chapter-3-sac-on-dense-reach","title":"Chapter 3: SAC on Dense Reach","text":"<p>Goal: Validate off-policy learning before adding HER.</p> <p>You will:</p> <ul> <li>Train SAC on FetchReachDense-v4 (same task as Chapter 2)</li> <li>Understand maximum-entropy RL and auto-tuned temperature</li> <li>Add replay buffer diagnostics (Q-values, entropy, goal distances)</li> <li>Benchmark throughput scaling with <code>n_envs</code></li> </ul> <p>Done when: SAC achieves &gt;95% success and you can answer:</p> <ul> <li>Why does SAC add an entropy bonus to the objective?</li> <li>What does it mean if Q-values grow unbounded?</li> <li>Why must we validate SAC separately before adding HER?</li> </ul>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li> Access to a machine with NVIDIA GPU</li> <li> Docker installed with NVIDIA Container Toolkit</li> <li> Basic Python and command-line familiarity</li> <li> This repository cloned locally</li> </ul>"},{"location":"tutorials/#how-to-use-these-tutorials","title":"How to Use These Tutorials","text":"<ol> <li>Work sequentially - Each chapter builds on the previous</li> <li>Run every command - Don't just read; execute</li> <li>Complete all deliverables - They verify your understanding</li> <li>Understand before proceeding - Speed is not the goal</li> </ol> <p>For the philosophical motivation behind this curriculum, see the full README.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/","title":"Chapter 0: A Containerized \"Proof of Life\" on Spark DGX","text":""},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#abstract","title":"Abstract","text":"<p>This chapter establishes the foundational experimental environment upon which all subsequent work depends. We address a problem that is logically prior to reinforcement learning itself: the problem of reproducible computation. Our deliverables are not trained policies but verified infrastructure--a container that runs, a renderer that produces images, a training loop that completes without error.</p> <p>The reader who dismisses this chapter as mere \"setup\" misunderstands its purpose. In empirical machine learning, the experimental environment is not scaffolding to be discarded; it is the laboratory in which results are produced. A result that cannot be reproduced because the environment cannot be reconstructed is not a result at all. This chapter ensures that our laboratory is sound.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#part-i-the-problem","title":"Part I: The Problem","text":""},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#11-why-the-reproducibility-crisis-and-its-resolution","title":"1.1 WHY: The Reproducibility Crisis and Its Resolution","text":"<p>Consider the following scenario, which is unfortunately common in empirical machine learning research. A researcher trains a policy that achieves impressive results. They write a paper, submit it, and it is accepted. Six months later, a colleague attempts to reproduce the results. The code is available, but it fails to run: dependencies have changed, the CUDA version is different, an environment variable is missing. After days of debugging, the colleague achieves results that are qualitatively different from the original paper. Was there an error in the original work? Or is the discrepancy due to environmental differences? It is impossible to know.</p> <p>This scenario illustrates what we might call the reproducibility problem in empirical machine learning:</p> <p>Problem (Reproducibility). Given a computational experiment \\(E\\) that produces result \\(R\\) on machine \\(M\\) at time \\(t\\), under what conditions can we guarantee that \\(E\\) produces result \\(R'\\) with \\(\\|R - R'\\| &lt; \\epsilon\\) on machine \\(M'\\) at time \\(t' &gt; t\\)?</p> <p>The problem is harder than it appears. The result \\(R\\) depends not only on the code but on the entire computational environment: the operating system, the installed libraries, the GPU driver version, the CUDA toolkit, the Python interpreter, and dozens of other components. Any of these may change between \\(t\\) and \\(t'\\), and any change may affect \\(R\\).</p> <p>In the tradition of Hadamard, we ask: Is the reproducibility problem well-posed?</p> <p>Definition (Well-Posedness for Reproducibility). The reproducibility problem is well-posed if: (1) there exists an environment specification \\(S\\) such that running \\(E\\) in any environment satisfying \\(S\\) produces consistent results; (2) the specification \\(S\\) is unique up to equivalence; (3) small perturbations to \\(S\\) produce small perturbations to \\(R\\).</p> <p>Condition (1) requires that we can specify an environment precisely enough to guarantee consistency. Condition (2) requires that the specification be canonical--that there not be multiple incompatible specifications claiming to represent the same environment. Condition (3) requires stability--that the result not be arbitrarily sensitive to minor environmental variations.</p> <p>Containerization addresses all three conditions. A Docker image provides a complete, self-contained specification of the computational environment. The image is identified by a content-addressable hash, ensuring uniqueness. And the layered filesystem ensures that small changes to the specification (adding a package, changing a configuration) produce small changes to the resulting environment.</p> <p>This is why we containerize: not for convenience, but for scientific validity.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#12-the-specific-problem-of-this-chapter","title":"1.2 The Specific Problem of This Chapter","text":"<p>Within the general reproducibility framework, this chapter addresses a specific sub-problem:</p> <p>Problem (Environment Verification). Construct and verify a containerized environment \\(S\\) such that: (1) GPU computation is available within \\(S\\); (2) MuJoCo physics simulation runs correctly within \\(S\\); (3) headless rendering produces valid images within \\(S\\); (4) a complete training loop executes without error within \\(S\\).</p> <p>Each condition is necessary for the reinforcement learning experiments that follow. Without GPU access, training is prohibitively slow. Without MuJoCo, we cannot simulate the Fetch robot. Without rendering, we cannot generate evaluation videos. Without a working training loop, we cannot learn policies.</p> <p>The verification is not optional. A researcher who skips this chapter and proceeds directly to training will eventually encounter failures--rendering errors, CUDA misconfigurations, import failures--and will spend more time debugging than if they had verified the environment systematically from the start.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#part-ii-the-method","title":"Part II: The Method","text":""},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#21-how-containerization-as-environment-specification","title":"2.1 HOW: Containerization as Environment Specification","text":"<p>Our approach is to specify the environment as a Docker container and to verify each requirement through explicit tests.</p> <p>Definition (Container). A container is a lightweight, isolated execution environment defined by an image. The image specifies the filesystem contents, environment variables, and default commands. Containers share the host kernel but have isolated process and network namespaces.</p> <p>Definition (Image). An image is an immutable template from which containers are instantiated. Images are identified by a content-addressable hash (digest) and may be tagged with human-readable names (e.g., <code>robotics-rl:latest</code>).</p> <p>The key property of containers for our purposes is isolation with specification. The container is isolated from the host environment--packages installed on the host do not affect the container--but the isolation is precisely specified by the image, which can be versioned, shared, and reconstructed.</p> <p>Remark (On the Choice of Docker). We use Docker rather than alternatives (Singularity, Podman, etc.) because Docker is ubiquitous, well-documented, and fully supported by the NVIDIA Container Toolkit. For HPC environments where Docker is unavailable, the concepts transfer to Singularity with minor modifications.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#22-the-verification-protocol","title":"2.2 The Verification Protocol","text":"<p>Verification is not bureaucracy. It is the empirical side of our well-posedness analysis. Each test corresponds to a necessary condition for the experiments that follow. If any test fails, some class of experiments becomes impossible. Understanding why each test matters--not just what it checks--is essential for diagnosing failures when they occur.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#test-1-gpu-access","title":"Test 1: GPU Access","text":"<p>What we verify. The container can access the host GPU via the NVIDIA runtime.</p> <p>Why this matters. Reinforcement learning with neural network function approximators is computationally intensive. A single training run may require \\(10^6\\)-\\(10^7\\) gradient updates, each involving forward and backward passes through networks with \\(10^5\\)-\\(10^6\\) parameters. On CPU, this takes days or weeks. On GPU, it takes hours.</p> <p>But GPU access inside a container is not automatic. The container runs in an isolated namespace; it cannot see host devices unless explicitly granted access. The <code>--gpus all</code> flag instructs Docker to use the NVIDIA Container Toolkit, which mounts the GPU device files and driver libraries into the container.</p> <p>What failure means. If this test fails, either: 1. The NVIDIA driver is not installed on the host 2. The NVIDIA Container Toolkit is not installed 3. Docker was not invoked with <code>--gpus all</code> 4. The GPU is in use by another process with exclusive access</p> <p>Training will still run on CPU, but it will be 10-100\u00d7 slower, making iterative experimentation impractical.</p> <p>The test. Run <code>nvidia-smi</code> inside the container. This command queries the NVIDIA driver for GPU status. If it succeeds, the container has GPU access. If it fails with \"command not found\" or \"NVIDIA-SMI has failed,\" the container cannot see the GPU.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#test-2-mujoco-and-gymnasium-robotics-functionality","title":"Test 2: MuJoCo and Gymnasium-Robotics Functionality","text":"<p>What we verify. The physics simulator initializes correctly, and the Fetch environments are registered and functional.</p> <p>Why this matters. The Fetch environments are implemented on top of MuJoCo, a physics engine that simulates rigid body dynamics with contact. MuJoCo is not a pure Python library; it includes compiled C code that interfaces with system libraries. If these libraries are missing or incompatible, MuJoCo fails to initialize.</p> <p>Furthermore, Gymnasium-Robotics must register its environments with Gymnasium's registry. This registration happens at import time. If the import fails silently or the registration is incomplete, <code>gym.make(\"FetchReach-v4\")</code> will raise <code>EnvironmentNameNotFound</code>.</p> <p>What failure means. If this test fails, either: 1. MuJoCo's compiled extensions cannot find required system libraries 2. The <code>gymnasium-robotics</code> package is not installed 3. There is a version incompatibility between <code>gymnasium</code>, <code>gymnasium-robotics</code>, and <code>mujoco</code></p> <p>Without functional Fetch environments, the entire curriculum is blocked.</p> <p>The test. Import <code>gymnasium</code> and <code>gymnasium_robotics</code>, then call <code>gym.make(\"FetchReachDense-v4\")</code> and <code>env.reset()</code>. If the environment returns a valid observation dictionary with keys <code>observation</code>, <code>achieved_goal</code>, and <code>desired_goal</code>, the physics stack is functional.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#test-3-headless-rendering","title":"Test 3: Headless Rendering","text":"<p>What we verify. The environment can produce RGB frames without a display.</p> <p>Why this matters. Evaluation often requires visual inspection: Does the robot reach the goal? Is the motion smooth or jerky? Does the gripper close at the right moment? These questions are answered by watching videos, which requires rendering.</p> <p>But DGX systems are headless--they have no monitor attached. Rendering typically requires a display server (X11) to manage the graphics context. On a headless system, we must use offscreen rendering: EGL (hardware-accelerated via the GPU) or OSMesa (software rasterization).</p> <p>This is where many setups fail. EGL requires specific driver support and library versions. OSMesa requires Mesa to be compiled with offscreen support. If neither works, rendering is impossible.</p> <p>What failure means. If this test fails, either: 1. EGL libraries (<code>libEGL.so</code>) are missing or incompatible 2. The GPU driver does not expose EGL support 3. OSMesa libraries (<code>libOSMesa.so</code>) are missing 4. Environment variables (<code>MUJOCO_GL</code>, <code>PYOPENGL_PLATFORM</code>) are misconfigured</p> <p>Training can proceed without rendering, but evaluation will be limited to numerical metrics. You will not be able to generate videos or visually debug policy behavior.</p> <p>The test. Create a Fetch environment with <code>render_mode=\"rgb_array\"</code>, call <code>env.render()</code>, and save the resulting numpy array as a PNG image. If the image file exists and is non-empty, offscreen rendering works.</p> <p>Remark (The Fallback Chain). The proof-of-life script implements a fallback chain: it first attempts EGL (preferred, hardware-accelerated), then OSMesa (slower, but compatible), then disables rendering entirely. The test passes if any backend produces a valid image.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#test-4-training-loop-completion","title":"Test 4: Training Loop Completion","text":"<p>What we verify. A complete training loop--environment interaction, gradient computation, parameter updates, checkpoint saving--executes without error.</p> <p>Why this matters. The previous tests verified components in isolation: GPU access, physics simulation, rendering. But reinforcement learning combines these components in complex ways. Data flows from the environment to the replay buffer to the neural network and back. Shapes must match. Dtypes must be compatible. Memory must not leak.</p> <p>Many bugs only manifest when components interact. A shape mismatch between the observation space and the policy network. A dtype incompatibility between numpy arrays and PyTorch tensors. A memory leak that only appears after thousands of environment steps. These bugs do not appear in unit tests; they appear when you run training.</p> <p>What failure means. If this test fails, either: 1. The policy network architecture is incompatible with the observation space 2. There is a dtype or device mismatch (CPU vs. CUDA tensors) 3. The training loop has a bug that manifests only after some number of steps 4. The checkpoint serialization format is incompatible with the model architecture</p> <p>Until this test passes, you cannot train policies.</p> <p>The test. Run PPO for 50,000 timesteps on <code>FetchReachDense-v4</code> with 8 parallel environments, then save a checkpoint. If the checkpoint file exists and is loadable by <code>PPO.load()</code>, the training loop is functional.</p> <p>Remark (Why PPO, Not SAC). We use PPO for this smoke test because it is simpler and fails faster if something is wrong. SAC involves additional components (replay buffer, twin critics, entropy tuning) that could mask or compound errors. Once PPO works, we have confidence that the core training infrastructure is sound; SAC-specific issues can be debugged separately.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#the-logical-structure","title":"The Logical Structure","text":"<p>The four tests form a dependency chain:</p> <pre><code>GPU Access \u2192 MuJoCo Functionality \u2192 Headless Rendering \u2192 Training Loop\n</code></pre> <p>Each test assumes the previous tests pass. There is no point testing rendering if MuJoCo cannot initialize. There is no point testing training if the GPU is inaccessible (training would \"work\" but be too slow to iterate).</p> <p>Run the tests in order. If a test fails, diagnose and fix it before proceeding. The proof-of-life script's <code>all</code> subcommand respects this ordering and stops at the first failure.</p> <p>These tests are implemented in <code>scripts/ch00_proof_of_life.py</code>. The script provides subcommands for running each test individually (<code>list-envs</code>, <code>render</code>, <code>ppo-smoke</code>) or all tests in sequence (<code>all</code>).</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#23-the-container-architecture","title":"2.3 The Container Architecture","text":"<p>Our container architecture consists of two layers:</p> <p>Base Layer. We use the NVIDIA PyTorch image (<code>nvcr.io/nvidia/pytorch:25.12-py3</code>) as the base. This image provides CUDA, cuDNN, PyTorch, and other deep learning infrastructure pre-configured and tested by NVIDIA.</p> <p>Project Layer. On top of the base, we install system dependencies for MuJoCo rendering (EGL, OSMesa) and Python dependencies for the project (Gymnasium, Gymnasium-Robotics, Stable Baselines 3). These are specified in the <code>docker/Dockerfile</code>, which builds the <code>robotics-rl:latest</code> image:</p> <ul> <li>System packages: <code>libegl1</code>, <code>libgl1</code>, <code>libosmesa6</code> (headless rendering), <code>libglfw3</code> (windowed rendering), <code>ffmpeg</code> (video encoding)</li> <li>Python packages: <code>gymnasium</code>, <code>gymnasium-robotics</code>, <code>mujoco</code>, <code>stable-baselines3</code>, <code>tensorboard</code>, <code>imageio</code>, <code>wandb</code></li> </ul> <p>The <code>docker/dev.sh</code> script automatically builds this image on first run if it does not exist locally. If the build fails (e.g., network issues), it falls back to the raw NVIDIA base image--but rendering may be unavailable in that case.</p> <p>Remark (On the Two-Layer Architecture). The separation into base and project layers reflects a design principle: heavyweight, stable dependencies (CUDA, PyTorch) belong in the base layer; lightweight, project-specific dependencies belong in the project layer. This separation enables faster iteration--changing project dependencies does not require rebuilding the entire CUDA stack--while maintaining reproducibility.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#24-the-virtual-environment-within-the-container","title":"2.4 The Virtual Environment Within the Container","text":"<p>A subtle point deserves elaboration. We use a Python virtual environment inside the container, even though the container already provides isolation.</p> <p>Proposition. A virtual environment inside a container provides additional benefits: (1) it enables <code>pip install -e .</code> for editable installs of the project; (2) it allows project dependencies to shadow container dependencies when necessary; (3) it makes the dependency specification explicit in <code>requirements.txt</code> rather than implicit in the Dockerfile.</p> <p>The virtual environment is created with <code>--system-site-packages</code>, which allows it to inherit packages from the container's system Python. This avoids reinstalling PyTorch (which is large and CUDA-specific) while still allowing project-specific packages to be installed separately.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#part-iii-the-implementation","title":"Part III: The Implementation","text":""},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#31-what-the-concrete-steps","title":"3.1 WHAT: The Concrete Steps","text":"<p>We now describe the concrete steps to establish and verify the environment.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#step-1-verify-host-prerequisites","title":"Step 1: Verify Host Prerequisites","text":"<p>Before using containers, we must verify that the host system is properly configured.</p> <p>Verification (Docker Availability). Run <code>docker --version</code> on the host. The test passes if Docker reports a version number.</p> <p>Verification (NVIDIA Runtime). Run <code>docker run --rm --gpus all nvcr.io/nvidia/pytorch:25.12-py3 nvidia-smi</code> on the host. The test passes if <code>nvidia-smi</code> output appears showing at least one GPU.</p> <p>If either test fails, consult your system administrator. This tutorial does not cover Docker installation or NVIDIA runtime configuration.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#step-2-enter-the-development-environment","title":"Step 2: Enter the Development Environment","text":"<p>The repository provides a wrapper script that automates container setup:</p> <pre><code>cd /home/vladp/src/robotics\nbash docker/dev.sh\n</code></pre> <p>This script performs the following operations:</p> <ol> <li>Checks for the <code>robotics-rl:latest</code> image; builds it if missing</li> <li>Launches a container with GPU access and appropriate environment variables</li> <li>Mounts the repository at <code>/workspace</code></li> <li>Runs as your host UID/GID to avoid permission issues</li> <li>Creates <code>.venv</code> if it does not exist and installs <code>requirements.txt</code></li> <li>Activates the virtual environment and drops you into a shell</li> </ol> <p>Alternatively, to run a single command without entering an interactive shell:</p> <pre><code>bash docker/dev.sh python scripts/ch00_proof_of_life.py all\n</code></pre>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#step-3-verify-fetch-environment-availability","title":"Step 3: Verify Fetch Environment Availability","text":"<p>Inside the container (or via <code>docker/dev.sh</code>), run:</p> <pre><code>python scripts/ch00_proof_of_life.py list-envs\n</code></pre> <p>Expected Output. A list of environment IDs including <code>FetchReach-v4</code>, <code>FetchReachDense-v4</code>, <code>FetchPush-v4</code>, <code>FetchPickAndPlace-v4</code>, and their variants.</p> <p>Failure Mode. If no Fetch environments appear, <code>gymnasium-robotics</code> is not installed correctly. Verify with <code>pip list | grep gymnasium</code>.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#step-4-verify-headless-rendering","title":"Step 4: Verify Headless Rendering","text":"<pre><code>python scripts/ch00_proof_of_life.py render --out smoke_frame.png\n</code></pre> <p>Expected Output. A message indicating successful rendering and the creation of <code>smoke_frame.png</code>.</p> <p>Failure Mode. If rendering fails with EGL errors, the script attempts fallback to OSMesa. If both fail, rendering is disabled. You can still train policies, but you cannot generate videos.</p> <p>Remark (Rendering Backend Hierarchy). The script implements a fallback chain: EGL (hardware-accelerated) \u2192 OSMesa (software) \u2192 disabled. EGL requires NVIDIA drivers and EGL libraries; OSMesa requires the Mesa library. The <code>robotics-rl:latest</code> image includes both.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#step-5-verify-training-loop","title":"Step 5: Verify Training Loop","text":"<pre><code>python scripts/ch00_proof_of_life.py ppo-smoke --n-envs 8 --total-steps 50000 --out ppo_smoke\n</code></pre> <p>Expected Output. Training progress messages followed by the creation of <code>ppo_smoke.zip</code>.</p> <p>Failure Mode. CUDA errors indicate GPU misconfiguration. Shape mismatch errors indicate problems with observation space handling. Import errors indicate missing dependencies.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#step-6-combined-verification","title":"Step 6: Combined Verification","text":"<p>For convenience, all tests can be run in sequence:</p> <pre><code>python scripts/ch00_proof_of_life.py all\n</code></pre> <p>This is the recommended way to verify a fresh installation.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#32-mac-m4-apple-silicon-support","title":"3.2 Mac M4 (Apple Silicon) Support","text":"<p>The platform also supports development on Apple Silicon Macs (M4, M3, M2, M1). The same <code>docker/dev.sh</code> command works on both platforms--the script auto-detects the host and configures appropriately.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#platform-differences","title":"Platform Differences","text":"Aspect DGX / NVIDIA Mac M4 (Apple Silicon) Architecture x86_64 ARM64 Docker image <code>robotics-rl:latest</code> <code>robotics-rl:mac</code> Dockerfile <code>docker/Dockerfile</code> <code>docker/Dockerfile.mac</code> Base image <code>nvcr.io/nvidia/pytorch:25.12-py3</code> <code>python:3.11-slim</code> Compute device CUDA (GPU) CPU only Rendering backend EGL (hardware) OSMesa (software) Typical throughput ~600 fps ~60-100 fps"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#why-cpu-only-on-mac","title":"Why CPU-Only on Mac?","text":"<p>Apple's Metal Performance Shaders (MPS) backend for PyTorch exists but has edge cases with certain operations. For maximum compatibility and to avoid subtle bugs, we use CPU on Mac. This is perfectly adequate for development and debugging--the physics simulation in MuJoCo is CPU-bound anyway.</p> <p>Remark (On Performance). The 6-10x slower throughput on Mac is expected. The bottleneck is MuJoCo physics simulation, which runs on CPU regardless of platform. On DGX, the GPU handles neural network operations in microseconds, so the CPU is the bottleneck. On Mac, both physics and neural networks run on CPU, compounding the slowdown. This is acceptable for development; use DGX for serious training runs.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#usage-on-mac","title":"Usage on Mac","text":"<p>The commands are identical:</p> <pre><code># Build image (auto-detects Mac, uses Dockerfile.mac)\nbash docker/build.sh\n\n# Run proof of life (auto-detects Mac, uses robotics-rl:mac)\nbash docker/dev.sh python scripts/ch00_proof_of_life.py all\n</code></pre> <p>The platform detection uses <code>uname -s</code> (Darwin for Mac) and <code>uname -m</code> (arm64 for Apple Silicon).</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#verification","title":"Verification","text":"<p>Run the same commands on each platform:</p> <pre><code># Build image (platform auto-detected)\nbash docker/build.sh\n\n# Run proof of life\nbash docker/dev.sh python scripts/ch00_proof_of_life.py all\n</code></pre> <p>Expected results by platform:</p> Aspect Mac M4 DGX / NVIDIA Device reported <code>cpu</code> <code>cuda</code> Rendering backend <code>osmesa</code> <code>egl</code> Training throughput ~60-100 fps ~600 fps <code>smoke_frame.png</code> Yes (Fetch robot visible) Yes (Fetch robot visible) <code>ppo_smoke.zip</code> Yes (loadable) Yes (loadable) All tests pass Yes Yes <p>All tests should pass on both platforms, and all artifacts should be generated correctly. The policies learned on Mac are interchangeable with those trained on DGX--the learned weights are platform-independent.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#known-limitations","title":"Known Limitations","text":"<ol> <li> <p>Performance: CPU training is ~10-20x slower than CUDA. Mac is suitable for development, debugging, and small experiments. For serious training (&gt;100k timesteps), use DGX.</p> </li> <li> <p>Rendering quality: OSMesa (software rendering) produces identical images to EGL but is slower. This matters only for video generation, not for training.</p> </li> <li> <p>Docker Desktop memory: You may need to increase Docker Desktop's memory allocation (Settings -&gt; Resources -&gt; Memory) to 8GB+ for large batch sizes or long training runs.</p> </li> <li> <p>No MPS (Apple GPU) support: MPS (Metal Performance Shaders) cannot work in Docker containers because Docker runs Linux, not macOS. MPS is a macOS-only API that requires direct access to Apple's Metal framework. Inside a Linux container, <code>torch.backends.mps.is_available()</code> always returns False, regardless of the host being an M4 Mac.</p> </li> </ol>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#docker-desktop-configuration-for-mac","title":"Docker Desktop Configuration for Mac","text":"<p>If you encounter out-of-memory errors:</p> <ol> <li>Open Docker Desktop</li> <li>Go to Settings (gear icon)</li> <li>Select \"Resources\"</li> <li>Increase \"Memory\" to at least 8 GB</li> <li>Click \"Apply &amp; restart\"</li> </ol>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#part-iv-analysis-and-verification","title":"Part IV: Analysis and Verification","text":""},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#41-interpreting-the-results","title":"4.1 Interpreting the Results","text":"<p>Upon successful completion of all tests, the following artifacts should exist:</p> Artifact Purpose Expected Properties <code>.venv/</code> Python virtual environment Contains <code>gymnasium</code>, <code>stable-baselines3</code>, etc. <code>smoke_frame.png</code> Rendered frame Non-empty PNG, shows Fetch robot <code>ppo_smoke.zip</code> Trained checkpoint Non-empty ZIP, loadable by SB3 <p>The existence and properties of these artifacts constitute empirical verification that the environment is correctly configured.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#42-on-the-meaning-of-proof-of-life","title":"4.2 On the Meaning of \"Proof of Life\"","text":"<p>The term \"proof of life\" is borrowed from hostage negotiations, where it refers to evidence that a hostage is still alive. In our context, it refers to evidence that the computational environment is functional.</p> <p>This is not mere metaphor. A misconfigured environment is, from the perspective of reproducible science, dead: it cannot produce results that can be trusted or reproduced. The tests in this chapter establish that our environment is alive--capable of producing valid, reproducible results.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#43-what-this-chapter-does-not-verify","title":"4.3 What This Chapter Does Not Verify","text":"<p>This chapter verifies that the environment is functional; it does not verify that training produces good policies. The PPO smoke test runs for only 50,000 timesteps, which is insufficient for convergence. The purpose is to confirm that the training loop executes, not that it produces useful results.</p> <p>Verification of learning performance is deferred to Chapter 2, where we establish PPO baselines with proper evaluation protocols.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#part-v-deliverables","title":"Part V: Deliverables","text":"<p>Upon completion of this chapter, the following must be true:</p> <p>D1. The command <code>bash docker/dev.sh</code> enters a container shell with an activated virtual environment.</p> <p>D2. The file <code>smoke_frame.png</code> exists and contains a valid image (viewable in an image viewer, showing the Fetch robot).</p> <p>D3. The file <code>ppo_smoke.zip</code> exists and is a valid Stable Baselines 3 checkpoint (loadable via <code>PPO.load(\"ppo_smoke.zip\")</code>).</p> <p>D4. All four tests in <code>scripts/ch00_proof_of_life.py all</code> pass without error.</p> <p>A reader who cannot satisfy all four conditions has not completed this chapter and should not proceed.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#appendix-a-troubleshooting","title":"Appendix A: Troubleshooting","text":""},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#a1-permission-denied-when-running-docker","title":"A.1 \"Permission denied\" When Running Docker","text":"<p>Symptom. <code>docker: Got permission denied while trying to connect to the Docker daemon socket.</code></p> <p>Cause. Your user is not in the <code>docker</code> group.</p> <p>Resolution. Either add your user to the <code>docker</code> group (<code>sudo usermod -aG docker $USER</code>, then log out and back in) or run Docker commands with <code>sudo</code>.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#a2-i-have-no-name-in-the-container-shell","title":"A.2 \"I have no name!\" in the Container Shell","text":"<p>Symptom. The shell prompt shows <code>I have no name!@&lt;container-id&gt;</code>.</p> <p>Cause. The container is running as your numeric UID, which has no entry in <code>/etc/passwd</code> inside the container.</p> <p>Impact. None. This is cosmetic. File permissions work correctly.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#a3-egl-initialization-failures","title":"A.3 EGL Initialization Failures","text":"<p>Symptom. <code>mujoco.FatalError: gladLoadGL error</code> or similar EGL errors.</p> <p>Cause. EGL libraries are missing or the GPU driver does not support EGL.</p> <p>Resolution. The <code>robotics-rl:latest</code> image includes EGL libraries. If using a different base image, install <code>libegl1</code> and <code>libgl1</code>. If EGL is unavailable, the script falls back to OSMesa.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#a4-dependency-hash-mismatch","title":"A.4 Dependency Hash Mismatch","text":"<p>Symptom. <code>docker/dev.sh</code> reinstalls packages on every run.</p> <p>Cause. The requirements hash file (<code>.venv/.requirements_hash</code>) is missing or corrupt.</p> <p>Resolution. Delete <code>.venv</code> and re-run <code>docker/dev.sh</code> to recreate the environment.</p>"},{"location":"tutorials/ch00_containerized_dgx_proof_of_life/#appendix-b-environment-variable-reference","title":"Appendix B: Environment Variable Reference","text":"Variable Value Purpose <code>MUJOCO_GL</code> <code>egl</code> Selects EGL rendering backend <code>PYOPENGL_PLATFORM</code> <code>egl</code> Configures PyOpenGL for EGL <code>NVIDIA_DRIVER_CAPABILITIES</code> <code>all</code> Enables full GPU access in container <p>These variables are set automatically by <code>docker/dev.sh</code>. If you launch containers manually, you must set them explicitly.</p> <p>Next. With the experimental environment verified, proceed to Chapter 1 to examine the structure of goal-conditioned Fetch environments.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/","title":"Chapter 1: The Anatomy of Goal-Conditioned Fetch Environments","text":""},{"location":"tutorials/ch01_fetch_env_anatomy/#abstract","title":"Abstract","text":"<p>This chapter develops a precise understanding of what a reinforcement learning agent \"perceives\" when interacting with a Gymnasium-Robotics Fetch environment. We formalize the observation space, action space, and reward function--not as implementation details to be glossed over, but as mathematical structures whose properties determine what algorithms can and cannot accomplish.</p> <p>The central result of this chapter is that Fetch environments implement a specific mathematical structure: the goal-conditioned Markov Decision Process. This structure--characterized by observation dictionaries with explicit goal representations and reward functions that can be evaluated for arbitrary goals--is not merely a design choice; it is the mathematical substrate that enables Hindsight Experience Replay and related techniques. A researcher who does not understand this structure cannot use those techniques correctly.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#part-0-the-practical-context","title":"Part 0: The Practical Context","text":""},{"location":"tutorials/ch01_fetch_env_anatomy/#01-where-we-are","title":"0.1 Where We Are","text":"<p>You have completed Chapter 0. You now have: - A working Docker container with GPU access - MuJoCo physics simulation running headlessly - A verified training loop that produces checkpoints</p> <p>You are running on a DGX cluster (or similar GPU workstation). Your environment is reproducible: anyone with the same Docker image and code can replicate your results.</p> <p>How to Execute Commands. All commands in this curriculum run through the Docker wrapper:</p> <pre><code>bash docker/dev.sh &lt;your-command&gt;\n</code></pre> <p>For example: <pre><code># Run a specific script\nbash docker/dev.sh python scripts/ch01_env_anatomy.py describe\n\n# Start an interactive shell inside the container\nbash docker/dev.sh\n</code></pre></p> <p>When you run <code>bash docker/dev.sh python some_script.py</code>, the following happens:</p> <ol> <li>Container launches with GPU access (<code>--gpus all</code>) and your repository mounted at <code>/workspace</code></li> <li>Virtual environment is created (first run) or activated in <code>.venv/</code></li> <li>Dependencies from <code>requirements.txt</code> are installed (cached by hash--reinstalls only when requirements change)</li> <li>Your command executes inside this isolated environment</li> <li>Container exits when the command completes (or stays open for interactive shells)</li> </ol> <p>The script preserves your host user ID, so files created inside the container are owned by you, not root.</p> <p>First Run. On first invocation, <code>dev.sh</code> builds the Docker image <code>robotics-rl:latest</code> from <code>docker/Dockerfile</code>. This takes several minutes but only happens once. Subsequent runs start in seconds.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#02-what-we-are-simulating","title":"0.2 What We Are Simulating","text":"<p>We are training neural network policies to control a simulated robot in a physics engine. Let us be precise about what this means.</p> <p>The Physics Engine: MuJoCo. MuJoCo (Multi-Joint dynamics with Contact) is a physics simulator designed for robotics and biomechanics research. It computes: - Rigid body dynamics (how objects move under forces) - Contact forces (what happens when the robot touches objects) - Joint constraints (how the robot's links connect)</p> <p>MuJoCo runs the physics at 500Hz internally; the environment exposes control at 25Hz (every 20 simulation steps). When you call <code>env.step(action)</code>, MuJoCo simulates 20 timesteps of physics, then returns the resulting state.</p> <p>Why Simulation? Reinforcement learning requires millions of trials. A real robot would: - Break from repeated collisions - Take months to collect enough data - Pose safety hazards during random exploration</p> <p>In simulation, we collect a million timesteps in minutes. Policies trained in simulation can later transfer to real hardware (sim-to-real transfer), though that is beyond this curriculum's scope.</p> <p>The Simulated Robot: Fetch. The Fetch robot is a real mobile manipulator manufactured by Fetch Robotics (now part of Zebra Technologies), designed for warehouse automation and research. The MuJoCo model in Gymnasium-Robotics replicates its kinematics.</p>   | Real Fetch Robot | Simulated FetchReach | |:----------------:|:--------------------:| | ![Fetch Robot](https://cdn.sanity.io/images/7p2whiua/production/f3dbbb6c9d9cf3ca435e531a7dba9750170f6ee3-2048x1536.jpg?w=400) | ![FetchReach](https://robotics.farama.org/_static/videos/fetch/reach.gif) | | *Source: [Robots Guide](https://robotsguide.com/robots/fetch/)* | *Source: [Gymnasium-Robotics](https://robotics.farama.org/envs/fetch/reach/)* |   <p>Real Robot Specifications (from Wevolver and Robots Guide):</p> Component Specification Arm 7 degrees of freedom, 940mm reach, 6kg payload Gripper Parallel-jaw, 245N grip force, swappable Joints Harmonic drives + brushless motors, 14-bit encoders Height 1.09m - 1.49m (telescoping spine) Weight 113 kg Compute Intel i5, Ubuntu + ROS <p>What We Simulate (the subset relevant for our tasks):</p> Component Simulation Detail Arm 7 DOF, matches real kinematics Gripper Parallel-jaw, 2 fingers Workspace ~1m reach from base Control mode Cartesian velocity (internal IK solver) Physics rate 500Hz internal, 25Hz control interface <p>The simulation includes a table with objects (for Push/PickAndPlace tasks) and a red sphere marking the goal position. The arm is mounted on a fixed base (we do not simulate the mobile platform).</p> <p>Further Reading: - Gymnasium-Robotics Fetch documentation -- official environment docs with action/observation specs - Original OpenAI Gym Robotics paper -- Plappert et al., \"Multi-Goal Reinforcement Learning\" (introduces these environments) - Fetch Robotics product page -- real robot specifications</p> <p>What the Agent Controls. The agent does not control joint torques directly. Instead, it outputs 4D Cartesian velocity commands: - <code>(vx, vy, vz)</code>: Desired end-effector velocity in world frame - <code>gripper</code>: Open (&lt;0) or close (&gt;0) command</p> <p>An internal controller (part of the MuJoCo model) converts these Cartesian commands to joint torques. This simplification means the agent does not need to learn inverse kinematics--it just says \"move left\" and the controller figures out which joints to actuate.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#03-what-we-are-doing","title":"0.3 What We Are Doing","text":"<p>Before we write any training code, we must understand exactly what the robot perceives and what commands it accepts.</p> <p>This is not academic pedantry. Consider what happens if you misunderstand the interface:</p> Misunderstanding Consequence Wrong observation shape Network architecture mismatch, cryptic shape errors Wrong action semantics Policy learns to output nonsense commands Wrong reward interpretation Hyperparameters tuned for wrong scale Missing goal structure Cannot use HER, forced to use inefficient methods <p>Every hour spent understanding the environment saves ten hours debugging training failures.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#04-the-four-fetch-tasks","title":"0.4 The Four Fetch Tasks","text":"<p>The Gymnasium-Robotics package provides four manipulation tasks of increasing difficulty:</p> Task Goal Difficulty FetchReach Move end-effector to target position Easiest--no object interaction FetchPush Push object to target position Medium--requires contact FetchPickAndPlace Pick up object, place at target Hard--requires grasping FetchSlide Slide object to distant target Hardest--requires throwing motion <p>Each task has two reward variants: - Dense (e.g., <code>FetchReachDense-v4</code>): Reward = negative distance to goal. Provides continuous feedback. - Sparse (e.g., <code>FetchReach-v4</code>): Reward = 0 if goal reached, \u22121 otherwise. Binary feedback only.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#05-why-environment-anatomy-matters","title":"0.5 Why Environment Anatomy Matters","text":"<p>The Fetch environments are not arbitrary. They implement a specific interface designed for goal-conditioned learning. Understanding this interface is essential because it enables a technique called Hindsight Experience Replay (HER) that we will use in Chapter 4.</p> <p>Here is the key insight, explained simply:</p> <p>The Problem with Sparse Rewards. Imagine you tell the robot \"reach position (0.5, 0.3, 0.2)\" but it ends up at (0.6, 0.4, 0.3). With sparse rewards, this trajectory gets reward = -1 at every step (failure). The robot learns nothing useful--it just knows it failed.</p> <p>The HER Solution. What if we could say: \"You failed to reach (0.5, 0.3, 0.2), but you successfully demonstrated how to reach (0.6, 0.4, 0.3)!\" We relabel the trajectory with the goal the robot actually achieved, recompute the rewards (now it's a success!), and learn from that.</p> <p>Why the Interface Matters. For this relabeling trick to work, the environment must provide three things:</p> <ol> <li>Separate goal information in observations.    The environment returns a dictionary with three keys:</li> <li><code>observation</code>: Robot state (joint positions, velocities)</li> <li><code>desired_goal</code>: Where we wanted to go (the target)</li> <li><code>achieved_goal</code>: Where we actually are (current position)</li> </ol> <p>Without this separation, we couldn't know what goal was \"achieved\" by a trajectory.</p> <ol> <li> <p>A <code>compute_reward()</code> function that accepts any goal. <pre><code>reward = env.unwrapped.compute_reward(achieved_goal, any_goal, info)\n</code></pre>    This lets us ask \"what would the reward have been if the goal were X?\" without re-running the simulation. This is how we recompute rewards after relabeling.</p> </li> <li> <p>A geometric success threshold.    Success means <code>distance(achieved_goal, desired_goal) &lt; 0.05</code> (5 centimeters). This concrete definition lets us determine success for any goal we choose to relabel with.</p> </li> </ol> <p>Bottom Line. These three interface features--dictionary observations, recomputable rewards, geometric success--are not arbitrary design choices. They are the mathematical substrate that makes HER possible. If any feature were missing, HER would not work.</p> <p>If you treat the environment as a black box--feeding observations to a network and hoping it learns--you will waste weeks on avoidable failures. This chapter makes the interface explicit so you can use HER correctly in Chapter 4 and debug intelligently when things go wrong.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#06-what-this-chapter-produces","title":"0.6 What This Chapter Produces","text":"<p>By the end of this chapter, you will have:</p> <ol> <li>JSON schemas documenting observation and action spaces exactly</li> <li>Verified reward consistency between <code>env.step()</code> and <code>compute_reward()</code></li> <li>Random baseline metrics establishing the performance floor</li> <li>Complete understanding of why Fetch environments enable HER</li> </ol> <p>These are not optional artifacts. They are the foundation on which all subsequent training rests.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#part-i-the-problem","title":"Part I: The Problem","text":""},{"location":"tutorials/ch01_fetch_env_anatomy/#11-why-the-goal-conditioning-paradigm","title":"1.1 WHY: The Goal-Conditioning Paradigm","text":"<p>Consider the problem of learning a robotic manipulation policy. A naive formulation might be:</p> <p>Naive Problem. Find a policy \\(\\pi: \\mathcal{S} \\to \\mathcal{A}\\) that maximizes expected cumulative reward on a manipulation task.</p> <p>This formulation is inadequate for two reasons.</p> <p>First, it conflates the policy with the task. A policy trained to reach position \\((0.5, 0.3, 0.2)\\) cannot generalize to position \\((0.6, 0.4, 0.3)\\) without retraining. If we want a policy that can reach arbitrary positions, we need the policy to accept the target position as input.</p> <p>Second, the naive formulation provides no mechanism for learning from failure. If the task is \"reach position \\(g\\)\" and the agent fails to reach \\(g\\), the episode provides no useful learning signal--the agent knows it failed, but not how to improve. This is particularly problematic with sparse rewards, where the agent receives no feedback until it succeeds.</p> <p>The goal-conditioned formulation resolves both issues:</p> <p>Problem (Goal-Conditioned Policy Learning). Let \\((\\mathcal{S}, \\mathcal{A}, \\mathcal{G}, P, R, \\gamma)\\) be a goal-conditioned MDP. Find a policy \\(\\pi: \\mathcal{S} \\times \\mathcal{G} \\to \\Delta(\\mathcal{A})\\) that maximizes:</p> \\[J(\\pi) = \\mathbb{E}_{g \\sim p(g)} \\mathbb{E}_{\\tau \\sim \\pi(\\cdot | \\cdot, g)} \\left[ \\sum_{t=0}^{T} \\gamma^t R(s_t, a_t, s_{t+1}, g) \\right]\\] <p>The policy now takes both state and goal as input, enabling generalization across goals. Moreover, the explicit goal structure enables relabeling: a trajectory that fails to reach goal \\(g\\) can be relabeled as a successful trajectory for whatever goal it actually reached, manufacturing learning signal from failure.</p> <p>This chapter examines how the Fetch environments implement this formulation.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#12-the-specific-questions-of-this-chapter","title":"1.2 The Specific Questions of This Chapter","text":"<p>We seek to answer four questions:</p> <p>Q1 (Observation Structure). What is the precise structure of the observation returned by <code>env.step()</code>? What are the shapes, ranges, and semantics of each component?</p> <p>Q2 (Action Semantics). What do actions mean? Are they joint torques, joint velocities, Cartesian velocities, or something else? What are their ranges and how are they clipped?</p> <p>Q3 (Reward Computation). How is the reward computed? Is it dense (continuous feedback) or sparse (binary success/failure)? Can the reward be recomputed for arbitrary goals?</p> <p>Q4 (Goal Achievement). How does the environment determine success? What is the <code>achieved_goal</code> and how does it relate to the <code>desired_goal</code>?</p> <p>These questions are not merely technical curiosities. The answers determine what algorithms are applicable, what hyperparameters are sensible, and what performance is achievable.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#part-ii-the-mathematical-framework","title":"Part II: The Mathematical Framework","text":""},{"location":"tutorials/ch01_fetch_env_anatomy/#21-how-the-goal-conditioned-mdp-formalism","title":"2.1 HOW: The Goal-Conditioned MDP Formalism","text":"<p>We begin with precise definitions.</p> <p>Definition (Goal-Conditioned MDP). A goal-conditioned Markov Decision Process is a tuple \\(\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{G}, P, R, \\phi, \\gamma)\\) where: - \\(\\mathcal{S}\\) is the state space - \\(\\mathcal{A}\\) is the action space - \\(\\mathcal{G}\\) is the goal space - \\(P: \\mathcal{S} \\times \\mathcal{A} \\to \\Delta(\\mathcal{S})\\) is the transition kernel - \\(R: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\times \\mathcal{G} \\to \\mathbb{R}\\) is the reward function - \\(\\phi: \\mathcal{S} \\to \\mathcal{G}\\) is the goal-achievement mapping - \\(\\gamma \\in [0, 1)\\) is the discount factor</p> <p>The goal-achievement mapping \\(\\phi\\) is crucial: it extracts from each state the \"achieved goal\"--the outcome that state represents. For a reaching task, \\(\\phi(s)\\) might be the end-effector position; for a pushing task, it might be the object position.</p> <p>Definition (Goal-Conditioned Observation). A goal-conditioned observation is a tuple \\(o = (\\bar{s}, g_a, g_d)\\) where: - \\(\\bar{s} \\in \\mathbb{R}^{d_s}\\) is the proprioceptive state (joint positions, velocities, etc.) - \\(g_a = \\phi(s) \\in \\mathcal{G}\\) is the achieved goal - \\(g_d \\in \\mathcal{G}\\) is the desired goal</p> <p>The separation of achieved and desired goals is what enables relabeling. Given a trajectory with observations \\((o_0, \\ldots, o_T)\\), we can substitute any \\(g' \\in \\mathcal{G}\\) for the desired goal and recompute rewards as \\(R(s_t, a_t, s_{t+1}, g')\\).</p> <p>Proposition (Reward Recomputation). If the reward function \\(R\\) depends on the goal only through the distance \\(\\|g_a - g_d\\|\\), then the reward for any achieved goal \\(g_a\\) and any hypothetical desired goal \\(g'\\) can be computed without re-simulating the trajectory:</p> \\[R(s, a, s', g') = f(\\|\\phi(s') - g'\\|)\\] <p>for some function \\(f: \\mathbb{R}_{\\geq 0} \\to \\mathbb{R}\\).</p> <p>This proposition is the mathematical foundation of Hindsight Experience Replay. It requires that the environment expose both \\(\\phi\\) (the goal-achievement mapping) and \\(R\\) (the reward function) in a form that allows evaluation for arbitrary goals.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#22-the-fetch-environment-as-goal-conditioned-mdp","title":"2.2 The Fetch Environment as Goal-Conditioned MDP","text":"<p>The Gymnasium-Robotics Fetch environments implement the goal-conditioned MDP structure as follows:</p> <p>State Space \\(\\mathcal{S}\\). The underlying state includes joint positions, joint velocities, gripper state, and (for manipulation tasks) object positions and velocities. The proprioceptive portion exposed to the agent has dimension \\(d_s = 10\\) for reaching tasks and \\(d_s = 25\\) for manipulation tasks.</p> <p>Action Space \\(\\mathcal{A}\\). Actions are 4-dimensional: \\(a = (v_x, v_y, v_z, g) \\in [-1, 1]^4\\). The first three components specify Cartesian velocity commands for the end-effector; the fourth controls the gripper (positive = close, negative = open).</p> <p>Goal Space \\(\\mathcal{G}\\). Goals are 3-dimensional Cartesian positions: \\(g \\in \\mathbb{R}^3\\). For reaching tasks, the goal is the target end-effector position; for manipulation tasks, it is the target object position.</p> <p>Reward Function \\(R\\). Two variants exist: - Dense: \\(R(s, a, s', g) = -\\|\\phi(s') - g\\|_2\\) - Sparse: \\(R(s, a, s', g) = \\begin{cases} 0 &amp; \\text{if } \\|\\phi(s') - g\\|_2 &lt; \\epsilon \\\\ -1 &amp; \\text{otherwise} \\end{cases}\\)</p> <p>where \\(\\epsilon = 0.05\\) is the success threshold.</p> <p>Goal-Achievement Mapping \\(\\phi\\). For reaching: \\(\\phi(s) = \\text{end-effector position}\\). For manipulation: \\(\\phi(s) = \\text{object position}\\).</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#23-the-dictionary-observation-structure","title":"2.3 The Dictionary Observation Structure","text":"<p>Fetch environments return observations as Python dictionaries with three keys:</p> <pre><code>{\n    'observation': np.ndarray,   # shape (d_s,), proprioceptive state\n    'achieved_goal': np.ndarray, # shape (3,), \u03c6(s)\n    'desired_goal': np.ndarray   # shape (3,), g\n}\n</code></pre> <p>Remark (Why Dictionaries?). The dictionary structure serves two purposes. First, it makes the goal-conditioned structure explicit--the achieved and desired goals are not buried in a flat observation vector but clearly labeled. Second, it enables automatic handling by Stable Baselines 3's <code>MultiInputPolicy</code>, which processes each key through a separate encoder before concatenation.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#24-the-compute_reward-function","title":"2.4 The compute_reward Function","text":"<p>Fetch environments expose a <code>compute_reward</code> method that allows reward evaluation for arbitrary goals:</p> <pre><code>reward = env.unwrapped.compute_reward(achieved_goal, desired_goal, info)\n</code></pre> <p>This method takes batches of achieved goals and desired goals and returns the corresponding rewards. It is the API through which HER implementations relabel trajectories.</p> <p>Critical Invariant. For any transition \\((s, a, s', g)\\), the following must hold:</p> \\[\\texttt{env.step(a)[1]} = \\texttt{env.unwrapped.compute\\_reward}(\\phi(s'), g, \\texttt{info})\\] <p>where the left side is the reward returned by <code>step()</code> and the right side is the reward computed by <code>compute_reward()</code>. Violation of this invariant would cause HER to learn from corrupted reward labels.</p> <p>This invariant is verified empirically in Section 3.3.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#part-iii-the-implementation","title":"Part III: The Implementation","text":""},{"location":"tutorials/ch01_fetch_env_anatomy/#31-what-empirical-verification-of-the-mathematical-structure","title":"3.1 WHAT: Empirical Verification of the Mathematical Structure","text":"<p>We now verify that the Fetch environments implement the goal-conditioned MDP structure as described. All verifications are implemented in <code>scripts/ch01_env_anatomy.py</code>.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#311-enumerating-available-environments","title":"3.1.1 Enumerating Available Environments","text":"<p>First, we enumerate the available Fetch environments:</p> <pre><code>bash docker/dev.sh python scripts/ch01_env_anatomy.py list-envs\n</code></pre> <p>Expected Output. Environment IDs including: - <code>FetchReach-v4</code> (sparse reaching) - <code>FetchReachDense-v4</code> (dense reaching) - <code>FetchPush-v4</code> (sparse pushing) - <code>FetchPushDense-v4</code> (dense pushing) - <code>FetchPickAndPlace-v4</code> (sparse pick-and-place) - <code>FetchPickAndPlaceDense-v4</code> (dense pick-and-place) - <code>FetchSlide-v4</code> (sparse sliding) - <code>FetchSlideDense-v4</code> (dense sliding)</p> <p>The naming convention encodes task and reward type: environments without \"Dense\" use sparse rewards; those with \"Dense\" use distance-based rewards.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#312-describing-observation-and-action-spaces","title":"3.1.2 Describing Observation and Action Spaces","text":"<pre><code>bash docker/dev.sh python scripts/ch01_env_anatomy.py describe --json-out results/ch01_env_describe.json\n</code></pre> <p>This command produces a JSON file documenting the precise structure of observations and actions.</p> <p>Expected Structure (FetchReach-v4):</p> <pre><code>{\n  \"action_space\": {\n    \"shape\": [4],\n    \"low\": [-1, -1, -1, -1],\n    \"high\": [1, 1, 1, 1]\n  },\n  \"observation_space\": {\n    \"observation\": {\"shape\": [10], \"low\": [...], \"high\": [...]},\n    \"achieved_goal\": {\"shape\": [3], \"low\": [...], \"high\": [...]},\n    \"desired_goal\": {\"shape\": [3], \"low\": [...], \"high\": [...]}\n  }\n}\n</code></pre> <p>Interpretation. The observation dictionary has three components as predicted by the theory. The action space is 4-dimensional and bounded by \\([-1, 1]\\). The goal spaces are 3-dimensional Cartesian coordinates.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#32-action-semantics","title":"3.2 Action Semantics","text":"<p>The 4-dimensional action vector is interpreted as follows:</p> Index Semantic Range Effect 0 \\(v_x\\) \\([-1, 1]\\) End-effector velocity in \\(x\\) 1 \\(v_y\\) \\([-1, 1]\\) End-effector velocity in \\(y\\) 2 \\(v_z\\) \\([-1, 1]\\) End-effector velocity in \\(z\\) 3 gripper \\([-1, 1]\\) Gripper command (\\(&gt;0\\) = close, \\(&lt;0\\) = open) <p>Remark (Cartesian vs. Joint Control). The Fetch environments use Cartesian velocity control, not joint torque control. This is a significant simplification: the agent does not need to learn inverse kinematics. The Cartesian commands are converted to joint commands by an internal controller that is part of the environment dynamics.</p> <p>Remark (Action Scaling). Actions are scaled by a factor before being applied. The exact scaling depends on the environment configuration. The agent outputs values in \\([-1, 1]\\); the environment scales these to physical units.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#33-reward-consistency-verification","title":"3.3 Reward Consistency Verification","text":"<p>We verify the critical invariant that <code>env.step()</code> rewards match <code>compute_reward()</code> rewards:</p> <pre><code>bash docker/dev.sh python scripts/ch01_env_anatomy.py reward-check --n-steps 500\n</code></pre> <p>Expected Output. A message confirming that all 500 reward comparisons matched within numerical tolerance.</p> <p>Failure Mode. If rewards do not match, HER will learn from incorrect reward labels. This would be a critical bug in either the environment or our understanding of its API.</p> <p>Remark (Why This Check Matters). The reward consistency check is not paranoid caution. Different versions of Gymnasium-Robotics have had bugs affecting reward computation. API changes have altered the signature of <code>compute_reward</code>. Running this check ensures that your specific installation behaves correctly.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#34-random-baseline-metrics","title":"3.4 Random Baseline Metrics","text":"<p>Before training any agent, we establish baseline performance with a random policy:</p> <pre><code>bash docker/dev.sh python scripts/ch01_env_anatomy.py random-episodes --n-episodes 10 --json-out results/ch01_random_metrics.json\n</code></pre> <p>Expected Output (FetchReachDense-v4): - <code>success_rate</code>: ~0.0-0.1 (random flailing occasionally reaches the goal) - <code>mean_return</code>: ~\u221215 to \u221225 (negative because rewards are negative distances) - <code>mean_episode_length</code>: 50 (environments truncate at 50 steps)</p> <p>Expected Output (FetchReach-v4, sparse): - <code>success_rate</code>: ~0.0-0.05 (very unlikely to reach by chance) - <code>mean_return</code>: ~\u221250 (constant \u22121 per step when not at goal)</p> <p>These baselines establish the performance floor. Any trained agent that does not significantly exceed random performance is not learning.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#part-iv-theoretical-implications","title":"Part IV: Theoretical Implications","text":""},{"location":"tutorials/ch01_fetch_env_anatomy/#41-why-the-dictionary-structure-enables-her","title":"4.1 Why the Dictionary Structure Enables HER","text":"<p>The dictionary observation structure is not arbitrary; it is the interface through which HER operates.</p> <p>Theorem (HER Applicability). Hindsight Experience Replay is applicable to an environment if and only if: 1. The observation includes an explicit <code>achieved_goal</code> \\(g_a = \\phi(s)\\) 2. The reward function \\(R(s, a, s', g)\\) can be evaluated for arbitrary goals \\(g\\) 3. The reward depends on the goal only through \\(\\|g_a - g\\|\\)</p> <p>The Fetch environments satisfy all three conditions by construction.</p> <p>Corollary. Standard (non-goal-conditioned) environments cannot use HER without modification. An environment that returns flat observations with no goal separation does not expose the structure HER requires.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#42-the-sparse-vs-dense-reward-trade-off","title":"4.2 The Sparse vs. Dense Reward Trade-off","text":"<p>The choice between sparse and dense rewards involves a fundamental trade-off:</p> <p>Dense Rewards: \\(R = -\\|g_a - g_d\\|\\) - Pro: Provides gradient signal at every timestep - Pro: Standard policy gradient methods (PPO) can learn effectively - Con: May encourage undesirable behaviors (e.g., hovering near the goal without reaching it) - Con: Reward shaping may not align with true task objective</p> <p>Sparse Rewards: \\(R = \\mathbf{1}[\\|g_a - g_d\\| &lt; \\epsilon] - 1\\) - Pro: Clearly defined success criterion - Pro: No reward shaping artifacts - Con: No gradient signal until goal is reached - Con: Requires HER or similar techniques for sample-efficient learning</p> <p>Remark (When to Use Each). For initial development and debugging, use dense rewards--they make it easier to verify that the pipeline is working. For final experiments, consider sparse rewards, which more accurately reflect the true task objective. HER bridges the gap by enabling sample-efficient learning even with sparse rewards.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#43-the-observation-dimension-and-policy-architecture","title":"4.3 The Observation Dimension and Policy Architecture","text":"<p>The observation dimensions have implications for policy architecture:</p> Environment <code>observation</code> dim <code>achieved_goal</code> dim <code>desired_goal</code> dim Total FetchReach 10 3 3 16 FetchPush 25 3 3 31 FetchPickAndPlace 25 3 3 31 <p>Stable Baselines 3's <code>MultiInputPolicy</code> handles dictionary observations by: 1. Processing each key through a separate MLP encoder 2. Concatenating the encoded representations 3. Passing the concatenation through shared layers</p> <p>This architecture allows the policy to learn separate representations for state and goal, which may improve generalization across goals.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#part-v-deliverables","title":"Part V: Deliverables","text":"<p>Upon completion of this chapter, the following must exist:</p> <p>D1. The file <code>results/ch01_env_describe.json</code> containing the observation and action space schema.</p> <p>D2. The reward consistency check (<code>bash docker/dev.sh python scripts/ch01_env_anatomy.py reward-check</code>) must pass.</p> <p>D3. The file <code>results/ch01_random_metrics.json</code> containing random baseline metrics.</p> <p>D4. The reader must be able to answer: - What is the dimension of the <code>observation</code> component for FetchReach? (Answer: 10) - What does action index 3 control? (Answer: gripper open/close) - What is the success threshold \\(\\epsilon\\)? (Answer: 0.05) - Why can HER relabel trajectories? (Answer: because <code>compute_reward</code> can evaluate arbitrary goals)</p> <p>A reader who cannot produce these deliverables or answer these questions has not completed the chapter.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#part-vi-connections","title":"Part VI: Connections","text":""},{"location":"tutorials/ch01_fetch_env_anatomy/#61-connection-to-chapter-0","title":"6.1 Connection to Chapter 0","text":"<p>This chapter assumes that the experimental environment from Chapter 0 is functional. All commands are executed inside the container via <code>docker/dev.sh</code>.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#62-connection-to-chapter-2","title":"6.2 Connection to Chapter 2","text":"<p>Chapter 2 will train a PPO baseline on <code>FetchReachDense-v4</code>. The observation structure documented here determines the policy architecture. The random baseline metrics establish the performance floor against which the trained agent is compared.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#63-connection-to-chapter-4","title":"6.3 Connection to Chapter 4","text":"<p>Chapter 4 will introduce HER for sparse-reward tasks. The <code>compute_reward</code> function verified here is the API through which HER relabels goals. The theoretical analysis of HER applicability in Section 4.1 explains why Fetch environments are suitable for HER and what properties an environment must have.</p>"},{"location":"tutorials/ch01_fetch_env_anatomy/#appendix-a-observation-component-details","title":"Appendix A: Observation Component Details","text":""},{"location":"tutorials/ch01_fetch_env_anatomy/#a1-fetchreach-observation-10-dimensions","title":"A.1 FetchReach Observation (10 dimensions)","text":"Index Semantic 0-2 Gripper position \\((x, y, z)\\) 3-5 Gripper velocity \\((\\dot{x}, \\dot{y}, \\dot{z})\\) 6-7 Gripper finger positions 8-9 Gripper finger velocities"},{"location":"tutorials/ch01_fetch_env_anatomy/#a2-fetchpushpickandplace-observation-25-dimensions","title":"A.2 FetchPush/PickAndPlace Observation (25 dimensions)","text":"Index Semantic 0-2 Gripper position 3-5 Object position 6-8 Object relative position (object \u2212 gripper) 9-11 Gripper velocity 12-14 Object velocity 15-17 Object relative velocity 18-21 Object rotation (quaternion) 22-24 Object angular velocity"},{"location":"tutorials/ch01_fetch_env_anatomy/#appendix-b-formal-verification-of-her-requirements","title":"Appendix B: Formal Verification of HER Requirements","text":"<p>Requirement 1: Explicit achieved goal. - Verified: Observations include <code>achieved_goal</code> key. - Test: <code>'achieved_goal' in env.observation_space.spaces</code></p> <p>Requirement 2: Computable reward for arbitrary goals. - Verified: <code>env.unwrapped.compute_reward(ag, dg, info)</code> accepts arbitrary goal arrays. - Test: Call with random goals, verify no errors.</p> <p>Requirement 3: Reward depends only on goal distance. - Verified: Dense reward is \\(-\\|g_a - g_d\\|\\); sparse is threshold on same. - Test: Verify <code>compute_reward(ag, dg, {})</code> equals <code>\u2212np.linalg.norm(ag \u2212 dg)</code> for dense.</p> <p>All three requirements are verified by <code>scripts/ch01_env_anatomy.py reward-check</code>.</p> <p>Next. With the environment anatomy understood, proceed to Chapter 2 to establish PPO baselines on dense Reach.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/","title":"Chapter 2: PPO on Dense Reach -- The Pipeline Truth Serum","text":""},{"location":"tutorials/ch02_ppo_dense_reach/#what-this-chapter-is-really-about","title":"What This Chapter Is Really About","text":"<p>Before we dive into math, let's be honest about what we're doing here.</p> <p>You're about to train a neural network to control a robot arm. The arm will learn to reach arbitrary 3D positions--not because someone programmed the kinematics, but because it tried millions of times and gradually figured out what works.</p> <p>The result: A neural network that figured out how to reach any point in 3D space.</p> <p></p> <p>No inverse kinematics. No trajectory planning. The robot learned this through 500,000 training steps--watch the distance counter drop to zero.</p> <p>That's remarkable. But here's the uncomfortable truth: most RL implementations don't work on the first try. The field has a reproducibility crisis (Henderson et al., 2018). Hyperparameters matter more than they should. Small bugs can silently cause complete failure.</p> <p>This chapter is about building confidence that your infrastructure is correct before you add complexity. We use Proximal Policy Optimization (PPO)--a widely-used RL algorithm that learns by repeatedly trying actions and adjusting based on outcomes--on a dense-reward task as a diagnostic. If this doesn't work, something is broken in your setup, not your algorithm.</p> <p>By the end, you will have: 1. A trained policy achieving &gt;90% success rate (we got 100% in our test run) 2. An understanding of why PPO works (not just that it works) 3. Diagnostic skills to identify common training failures 4. Confidence to move to harder problems</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#part-0-setting-the-stage","title":"Part 0: Setting the Stage","text":""},{"location":"tutorials/ch02_ppo_dense_reach/#01-the-problem-were-solving","title":"0.1 The Problem We're Solving","text":"<p>Imagine you want to teach a robot arm to touch a target. You could:</p> <p>Option A: Program it explicitly. Compute inverse kinematics, plan a trajectory, execute. This works but requires knowing the robot's geometry precisely and doesn't generalize to new situations.</p> <p>Option B: Let it learn. Show the robot where the target is, let it flail around, reward it when it gets close. Over time, it figures out how to reach any target.</p> <p>Option B is reinforcement learning. It's harder to get working, but once it works, the same algorithm can learn to push objects, pick things up, even walk--without you programming each behavior explicitly.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#02-why-start-here","title":"0.2 Why Start Here?","text":"<p>The Fetch robot in simulation has 7 joints and a gripper. The full task hierarchy looks like:</p> Task Difficulty What Makes It Hard Reach Easiest Just move the end-effector to a point Push Medium Must contact and move an object Pick &amp; Place Hard Must grasp, lift, and place accurately <p>We start with Reach because it isolates the core RL problem: learning a mapping from observations to actions. No object dynamics, no contact physics, no grasp planning. Just: \"see goal, move there.\"</p> <p>And we use dense rewards (negative distance to goal) rather than sparse rewards (success/failure only). This gives the learning algorithm continuous feedback--every action either improves or worsens the situation.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#03-the-diagnostic-mindset","title":"0.3 The Diagnostic Mindset","text":"<p>Here's a scenario that happens more often than anyone admits:</p> <p>You implement an advanced algorithm on a difficult task. Train for 10 hours. Success rate: 0%. What went wrong?</p> <p>The honest answer: you have no idea. It could be: - Environment misconfiguration - Wrong network architecture - Bad hyperparameters - Bug in your algorithm implementation - The task simply needing more training time - A subtle numerical issue</p> <p>You're debugging in the dark with too many variables.</p> <p>The solution: Establish a baseline where failure is informative. Start with the simplest algorithm (PPO) on the easiest task (Reach with dense rewards). If this doesn't work, the problem is in your infrastructure, not your algorithm choice.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#part-1-why-understanding-the-learning-problem","title":"Part 1: WHY -- Understanding the Learning Problem","text":""},{"location":"tutorials/ch02_ppo_dense_reach/#11-what-are-we-actually-optimizing","title":"1.1 What Are We Actually Optimizing?","text":"<p>Let's build up the math from intuition, defining each symbol as we introduce it.</p> <p>The Setup: At each timestep \\(t\\), our policy \\(\\pi\\) (a neural network with parameters \\(\\theta\\)) sees the current state \\(s_t\\) and goal \\(g\\), and outputs an action \\(a_t\\). The environment responds with a new state \\(s_{t+1}\\) and a reward \\(R_t\\)--a single number indicating how good that transition was.</p> <p>Before stating the objective, we need three definitions:</p> <p>Definition (Reward). The reward \\(R_t \\in \\mathbb{R}\\) is the immediate feedback signal at timestep \\(t\\). In FetchReachDense, \\(R_t = -\\|p_t - g\\|\\) where \\(p_t\\) is the gripper position and \\(g\\) is the goal. More negative means farther from the goal; zero means perfect.</p> <p>Definition (Discount Factor). The discount factor \\(\\gamma \\in [0, 1)\\) determines how much we value future rewards relative to immediate rewards. A reward \\(R\\) received \\(k\\) steps in the future contributes \\(\\gamma^k R\\) to our objective. With \\(\\gamma = 0.99\\), a reward 100 steps away is worth \\(0.99^{100} \\approx 0.37\\) as much as an immediate reward. This captures two intuitions: (1) sooner is better than later, and (2) distant rewards are more uncertain.</p> <p>Definition (Time Horizon). The horizon \\(T\\) is the maximum number of timesteps in an episode. For FetchReach, \\(T = 50\\) steps.</p> <p>The Objective: Find policy parameters \\(\\theta\\) that maximize the expected discounted return:</p> <pre><code>J(\\theta) = \\mathbb{E}\\left[ \\sum_{t=0}^{T} \\gamma^t R_t \\right]\n</code></pre> <p>Here \\(J(\\theta)\\) is the objective function we seek to maximize--it measures how good a policy with parameters \\(\\theta\\) is, averaged over many episodes. The sum \\(\\sum_{t=0}^{T} \\gamma^t R_t\\) is called the return: the total reward accumulated over an episode, with future rewards discounted by \\(\\gamma\\).</p> <p>The expectation is over trajectories--different runs give different outcomes because actions sample from the policy distribution and the environment may be stochastic.</p> <p>The Challenge: How do you take a gradient of this? The expectation depends on \\(\\theta\\) in a complicated way--\\(\\theta\\) determines the policy, which determines the actions, which determines the states visited, which determines the rewards.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#12-the-policy-gradient-theorem-intuition-first","title":"1.2 The Policy Gradient Theorem (Intuition First)","text":"<p>Here's the key insight, stated informally:</p> <p>To improve the policy, increase the probability of actions that led to better-than-expected outcomes, and decrease the probability of actions that led to worse-than-expected outcomes.</p> <p>The \"better-than-expected\" part is crucial. An action that got reward +10 isn't necessarily good--if you typically get +15 from that state, it was actually a bad choice.</p> <p>This is captured by the advantage function:</p> <pre><code>A(s, a) = Q(s, a) - V(s)\n</code></pre> <p>where: - \\(Q(s, a)\\) = expected return if you take action \\(a\\) in state \\(s\\), then follow your policy - \\(V(s)\\) = expected return if you just follow your policy from state \\(s\\)</p> <p>So \\(A(s, a) &gt; 0\\) means action \\(a\\) was better than average; \\(A(s, a) &lt; 0\\) means it was worse.</p> <p>The Policy Gradient:</p> <pre><code>\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[ \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot A(s_t, a_t) \\right]\n</code></pre> <p>Read this as: \"Adjust \\(\\theta\\) to make good actions more likely and bad actions less likely, weighted by how good/bad they were.\"</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#13-the-problem-policy-gradient-is-unstable","title":"1.3 The Problem: Policy Gradient Is Unstable","text":"<p>In theory, you can just follow this gradient and improve. In practice, vanilla policy gradient is notoriously unstable. Here's why:</p> <p>Problem 1: Advantage estimates are noisy.</p> <p>We don't know the true advantage--we estimate it from sampled trajectories. With a finite batch, these estimates have high variance. Sometimes they're way off, and we make bad updates.</p> <p>Problem 2: Big updates break everything.</p> <p>Suppose we estimate that some action is great (\\(A &gt;&gt; 0\\)) and crank up its probability. But if our estimate was wrong, we've now committed to a bad action. Worse, the new policy visits different states, making our old advantage estimates invalid. The whole thing can spiral into catastrophic collapse.</p> <p>This isn't hypothetical--it happens all the time. Training curves that look promising suddenly crash to zero and never recover.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#14-ppos-solution-constrained-updates","title":"1.4 PPO's Solution: Constrained Updates","text":"<p>PPO's key idea: don't change the policy too much in one update.</p> <p>But \"too much\" in what sense? Not in parameter space (a small parameter change can cause large behavior change). Instead, in probability space.</p> <p>Define the probability ratio:</p> <pre><code>r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}\n</code></pre> <p>This measures how the action probability changed: - \\(r = 1\\): same probability as before - \\(r = 2\\): action is now twice as likely - \\(r = 0.5\\): action is now half as likely</p> <p>PPO clips this ratio to stay in \\([1-\\epsilon, 1+\\epsilon]\\) (typically \\(\\epsilon = 0.2\\)):</p> <pre><code>L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min\\left( r_t A_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) \\cdot A_t \\right) \\right]\n</code></pre> <p>What this does:</p> Advantage Gradient wants to... Clipping effect \\(A &gt; 0\\) (good action) Increase \\(r\\) (make action more likely) Stops at \\(r = 1.2\\) \\(A &lt; 0\\) (bad action) Decrease \\(r\\) (make action less likely) Stops at \\(r = 0.8\\) <p>The policy can improve, but only within a \"trust region\" around its current behavior. This prevents the catastrophic updates that kill vanilla policy gradient.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#15-a-concrete-example","title":"1.5 A Concrete Example","text":"<p>Let's trace through one update to make this concrete.</p> <p>Setup: The old policy assigns probability 0.3 to action \\(a\\) in state \\(s\\). We estimate the advantage is \\(A = +2\\) (this was a good action).</p> <p>Naive approach: The gradient says \"make this action more likely!\" So we update and now \\(\\pi_\\theta(a|s) = 0.6\\).</p> <p>Problem: The ratio \\(r = 0.6/0.3 = 2.0\\). We doubled the probability in one update. If our advantage estimate was wrong, we've made a big mistake.</p> <p>PPO's approach: The clipped objective computes: - Unclipped: \\(r \\cdot A = 2.0 \\times 2 = 4.0\\) - Clipped: \\(\\text{clip}(2.0, 0.8, 1.2) \\times 2 = 1.2 \\times 2 = 2.4\\) - Objective: \\(\\min(4.0, 2.4) = 2.4\\)</p> <p>The gradient only flows through the clipped version. We still increase the action probability, but the update is bounded. We can't go from 0.3 to 0.6 in one step--we'd need multiple updates, each constrained.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#16-why-dense-rewards-matter-here","title":"1.6 Why Dense Rewards Matter Here","text":"<p>FetchReachDense-v4 gives reward \\(R = -\\|achieved - goal\\|_2\\) at every step. This is the negative distance to the goal.</p> <p>Why this helps: - Every action provides signal: \"you got 2cm closer\" or \"you drifted 1cm away\" - The learning algorithm always has gradient information - Exploration isn't a bottleneck--even random actions provide useful data</p> <p>Compare to sparse rewards (\\(R = 0\\) if success, \\(-1\\) otherwise): - You only learn when you succeed - Random policy almost never succeeds - Most of your data is uninformative</p> <p>Dense rewards decouple the exploration problem from the learning problem. If PPO fails on dense Reach, the issue is definitely in your implementation, not in insufficient exploration.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#part-2-how-the-algorithm-in-detail","title":"Part 2: HOW -- The Algorithm in Detail","text":""},{"location":"tutorials/ch02_ppo_dense_reach/#21-the-actor-critic-architecture","title":"2.1 The Actor-Critic Architecture","text":"<p>PPO maintains two neural networks:</p> <p>Actor \\(\\pi_\\theta(a|s, g)\\): Given the state and goal, output a probability distribution over actions. For continuous actions, this is typically a Gaussian with learned mean and standard deviation.</p> <p>Critic \\(V_\\phi(s, g)\\): Given the state and goal, estimate the expected return. This helps compute advantages.</p> <p>Why two networks, not one? A natural question: why not have a single network output both actions and value estimates? Three reasons:</p> <ol> <li> <p>Different objectives. The actor maximizes expected return (wants to find good actions). The critic minimizes prediction error (wants accurate value estimates). These gradients can conflict--improving one may hurt the other.</p> </li> <li> <p>Different output types. The actor outputs a probability distribution (mean and variance for continuous actions). The critic outputs a single scalar. Forcing these through the same final layers creates unnecessary coupling.</p> </li> <li> <p>Stability. The critic's value estimates are used to compute advantages, which then train the actor. If actor updates destabilize the critic, the advantages become noisy, which destabilizes the actor further--a vicious cycle.</p> </li> </ol> <p>A geometric perspective: There may be deeper structure here. Consider what we're learning: a mapping from states to \"optimal behavior,\" encompassing both what to do (policy) and how good is this state (value). Naively, this is a single map:</p> \\[F: \\mathcal{S} \\times \\mathcal{G} \\to \\mathcal{P}(\\mathcal{A}) \\times \\mathbb{R}\\] <p>Actor-critic separates this into two maps:</p> \\[\\pi: \\mathcal{S} \\times \\mathcal{G} \\to \\mathcal{P}(\\mathcal{A}) \\quad \\text{and} \\quad V: \\mathcal{S} \\times \\mathcal{G} \\to \\mathbb{R}\\] <p>This suggests a factorization--perhaps recognizing product structure in the target space, or projecting through a lower-dimensional \"behaviorally-relevant\" manifold. The shared backbone makes this more concrete: we learn \\(\\phi: \\mathcal{S} \\times \\mathcal{G} \\to \\mathcal{Z}\\) (a representation), then compose with separate heads. This is genuinely factoring through an intermediate space.</p> <p>However, the analogy is imperfect. Unlike clean mathematical factorizations, \\(V\\) depends on \\(\\pi\\) (it's \\(V^\\pi\\)), so the components are coupled. The precise geometric interpretation--if one exists--remains to be clarified. What's clear is that the separation has practical benefits; whether it reflects deep structure or is merely a useful engineering heuristic is an open question.</p> <p>In practice, implementations often share early layers (a \"backbone\") with separate final layers (\"heads\"). This captures shared features while keeping the objectives separate. Stable Baselines 3 uses this approach by default.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#22-the-training-loop","title":"2.2 The Training Loop","text":"<pre><code>repeat:\n    1. Collect N steps using current policy\n    2. Compute advantages using critic\n    3. Update actor using clipped objective (multiple epochs)\n    4. Update critic using MSE loss on returns\n    5. Discard data, go to 1\n</code></pre> <p>Step 1: Collect Data</p> <p>Run the policy for <code>n_steps</code> in each of <code>n_envs</code> parallel environments. This gives us <code>n_steps * n_envs</code> transitions to learn from.</p> <p>Step 2: Compute Advantages</p> <p>We use Generalized Advantage Estimation (GAE), which balances bias and variance:</p> <pre><code>\\hat{A}_t = \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k \\delta_{t+k}\n</code></pre> <p>where \\(\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\\) is the TD residual.</p> <p>The parameter \\(\\lambda\\) interpolates between: - \\(\\lambda = 0\\): One-step TD (high bias, low variance) - \\(\\lambda = 1\\): Monte Carlo (low bias, high variance) - \\(\\lambda = 0.95\\): Typical default</p> <p>Steps 3-4: Update Networks</p> <p>Unlike supervised learning, we do multiple passes over the same data: - <code>n_epochs = 10</code> is typical for PPO - Each pass uses minibatches of size <code>batch_size</code></p> <p>This reuses our expensive-to-collect trajectory data while the clipping prevents us from overfitting to it.</p> <p>Step 5: Discard and Repeat</p> <p>PPO is on-policy: we can only use data from the current policy. After updating, our data is \"stale\" and must be discarded.</p> <p>This is inefficient compared to off-policy methods (which reuse old data), but simpler and more stable.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#23-key-hyperparameters","title":"2.3 Key Hyperparameters","text":"Parameter Our Setting What It Controls <code>n_steps</code> 1024 Trajectory length before update <code>n_envs</code> 8 Parallel environments (throughput) <code>batch_size</code> 256 Minibatch size for gradient updates <code>n_epochs</code> 10 Passes over data per update <code>learning_rate</code> 3e-4 Gradient step size <code>clip_range</code> 0.2 PPO clipping parameter (\\(\\epsilon\\)) <code>gae_lambda</code> 0.95 Advantage estimation bias-variance <code>ent_coef</code> 0.0 Entropy bonus (exploration) <p>For FetchReachDense-v4, Stable Baselines 3 defaults work well. Don't tune hyperparameters until you've verified the baseline works.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#part-3-what-running-the-experiment","title":"Part 3: WHAT -- Running the Experiment","text":""},{"location":"tutorials/ch02_ppo_dense_reach/#31-the-one-command-version","title":"3.1 The One-Command Version","text":"<pre><code>bash docker/dev.sh python scripts/ch02_ppo_dense_reach.py all --seed 0\n</code></pre> <p>This runs training, evaluation, and generates a report. Takes ~6-10 minutes on a GPU.</p> <p>For a quick sanity check (~1 minute):</p> <pre><code>bash docker/dev.sh python scripts/ch02_ppo_dense_reach.py train --total-steps 50000\n</code></pre>"},{"location":"tutorials/ch02_ppo_dense_reach/#32-what-to-expect","title":"3.2 What to Expect","text":"<p>Training progress (watch for these milestones):</p> Timesteps Success Rate What's Happening 0-50k 5-10% Random exploration 50k-100k 30-50% Policy starting to learn 100k-200k 70-90% Rapid improvement 200k-500k 95-100% Fine-tuning, convergence <p>Our test run achieved: - 100% success rate after 500k steps - 4.6mm average goal distance (environment considers &lt;50mm as success) - ~1300 steps/second throughput on NVIDIA GB10</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#33-reading-the-tensorboard-logs","title":"3.3 Reading the TensorBoard Logs","text":"<p>Launch TensorBoard:</p> <pre><code>bash docker/dev.sh tensorboard --logdir runs --bind_all\n</code></pre> <p>Healthy training looks like:</p> Metric Expected Behavior <code>rollout/ep_rew_mean</code> Steadily increasing (less negative) <code>rollout/success_rate</code> 0 -&gt; 1 over training <code>train/value_loss</code> High initially, decreases, stabilizes <code>train/approx_kl</code> Small (&lt; 0.03), occasional spikes OK <code>train/clip_fraction</code> 0.1-0.3 (some updates clipped, not all) <code>train/entropy_loss</code> Slowly decreasing (policy becoming more deterministic) <p>Warning signs:</p> Symptom Likely Problem Fix <code>ep_rew_mean</code> flatlines at start Environment misconfigured Check obs/action shapes <code>value_loss</code> explodes Reward scale wrong Check reward range <code>approx_kl</code> consistently &gt; 0.05 Learning rate too high Reduce to 1e-4 <code>clip_fraction</code> near 1.0 Updates too aggressive Reduce LR or clip_range <code>entropy_loss</code> immediately 0 Policy collapsed Increase ent_coef"},{"location":"tutorials/ch02_ppo_dense_reach/#34-verifying-your-results","title":"3.4 Verifying Your Results","text":"<p>After training completes, check the evaluation report:</p> <pre><code>cat results/ch02_ppo_fetchreachdense-v4_seed0_eval.json | python -m json.tool | head -20\n</code></pre> <p>Key fields: <pre><code>{\n  \"aggregate\": {\n    \"success_rate\": 1.0,\n    \"return_mean\": -0.40,\n    \"final_distance_mean\": 0.0046\n  }\n}\n</code></pre></p> <p>Passing criteria: - Success rate &gt; 90% - Mean return &gt; -10 - Final distance &lt; 0.02m</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#part-4-understanding-what-you-built","title":"Part 4: Understanding What You Built","text":""},{"location":"tutorials/ch02_ppo_dense_reach/#41-what-the-policy-actually-learned","title":"4.1 What the Policy Actually Learned","text":"<p>The trained policy maps observations to actions:</p> <p>Input (25 dimensions total): - End-effector position (3) - End-effector velocity (3) - Gripper state (4) - Desired goal (3) - Achieved goal (3) - Various other features (9)</p> <p>Output (4 dimensions): - dx, dy, dz: Cartesian velocity commands - gripper: open/close command</p> <p>The network learned that to reach a goal, it should output velocities that point toward the goal. This seems obvious, but the network discovered it purely from trial and error.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#42-the-clipping-in-action","title":"4.2 The Clipping in Action","text":"<p>During training, you can observe the clipping mechanism working:</p> <ul> <li><code>clip_fraction = 0.15</code> means 15% of updates were clipped</li> <li>This is healthy--some updates are constrained, preventing instability</li> <li><code>clip_fraction = 0</code> would mean the policy isn't learning aggressively enough</li> <li><code>clip_fraction = 1.0</code> would mean all updates are being constrained (too aggressive)</li> </ul>"},{"location":"tutorials/ch02_ppo_dense_reach/#43-why-this-validates-your-pipeline","title":"4.3 Why This Validates Your Pipeline","text":"<p>If PPO succeeds on dense Reach, you know:</p> <ol> <li>Environment is configured correctly - Observations and actions have the right shapes and semantics</li> <li>Network architecture works - MultiInputPolicy correctly processes dict observations</li> <li>GPU acceleration works - Training completes in reasonable time</li> <li>Evaluation protocol is sound - You can load checkpoints and run deterministic rollouts</li> <li>Metrics are computed correctly - Success rate matches what you observe</li> </ol> <p>Now you can add complexity (SAC, HER, harder tasks) with confidence that failures are algorithmic, not infrastructural.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#part-5-exercises","title":"Part 5: Exercises","text":""},{"location":"tutorials/ch02_ppo_dense_reach/#exercise-21-reproduce-the-baseline","title":"Exercise 2.1: Reproduce the Baseline","text":"<p>Run training with seed 0 and verify you achieve &gt; 90% success rate. Record: - Final success rate - Final mean return - Training time - Steps per second</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#exercise-22-multi-seed-validation","title":"Exercise 2.2: Multi-Seed Validation","text":"<p>Run with seeds 0-4 and compute mean and standard deviation:</p> <pre><code>bash docker/dev.sh python scripts/ch02_ppo_dense_reach.py multi-seed --seeds 5\n</code></pre> <p>A robust result should have std &lt; 5%.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#exercise-23-explain-the-clipping-written","title":"Exercise 2.3: Explain the Clipping (Written)","text":"<p>Answer these questions in your own words:</p> <ol> <li>What problem does vanilla policy gradient have that PPO fixes?</li> <li>Why does PPO clip in probability ratio space rather than parameter space?</li> <li>If \\(\\epsilon = 0\\) (no change allowed), what would happen?</li> <li>If \\(\\epsilon = 1\\) (large changes allowed), what would happen?</li> </ol>"},{"location":"tutorials/ch02_ppo_dense_reach/#exercise-24-ablation-study","title":"Exercise 2.4: Ablation Study","text":"<p>Train with <code>clip_range</code> values of 0.1, 0.2, and 0.4. For each: - Does final performance change? - Does training stability change? - How does <code>clip_fraction</code> in TensorBoard differ?</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#part-6-common-failures-and-solutions","title":"Part 6: Common Failures and Solutions","text":""},{"location":"tutorials/ch02_ppo_dense_reach/#success-rate-stays-at-0","title":"\"Success rate stays at 0%\"","text":"<p>Check 1: Environment shape <pre><code>import gymnasium as gym\nenv = gym.make(\"FetchReachDense-v4\")\nobs, _ = env.reset()\nprint(\"Obs shape:\", {k: v.shape for k, v in obs.items()})\nprint(\"Action shape:\", env.action_space.shape)\n</code></pre></p> <p>Expected: obs has keys with shapes (10,), (3,), (3,); action shape (4,).</p> <p>Check 2: Reward range <pre><code>for _ in range(100):\n    action = env.action_space.sample()\n    obs, reward, _, _, _ = env.step(action)\n    print(f\"Reward: {reward:.3f}\")\n</code></pre></p> <p>Expected: rewards in roughly [-1, 0] range.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#value-loss-explodes","title":"\"Value loss explodes\"","text":"<p>Usually means reward scale is wrong. FetchReachDense returns rewards in [-1, 0]. If you're seeing rewards in [-1000, 0] or similar, something is misconfigured.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#training-is-slow-500-fps","title":"\"Training is slow (&lt;500 fps)\"","text":"<p>Check that GPU is being used: <pre><code>nvidia-smi  # Should show python process using GPU memory\n</code></pre></p> <p>Check that you're running in Docker with <code>--gpus all</code>: <pre><code>bash docker/dev.sh nvidia-smi\n</code></pre></p>"},{"location":"tutorials/ch02_ppo_dense_reach/#conclusion","title":"Conclusion","text":"<p>This chapter established something more important than a trained policy: confidence in your infrastructure.</p> <p>The \"truth serum\" principle says: before tackling hard problems, verify that easy problems work. PPO on dense Reach is that easy problem. With 100% success rate achieved, we know our environment, training loop, evaluation protocol, and GPU setup are correct.</p> <p>Key takeaways:</p> <ol> <li>PPO prevents catastrophic updates through clipped probability ratios</li> <li>Dense rewards provide continuous signal, decoupling exploration from learning</li> <li>Diagnostics reveal problems early--watch TensorBoard, not just final metrics</li> <li>Infrastructure bugs are silent killers--validate before adding complexity</li> </ol> <p>What's Next:</p> <p>Chapter 3 introduces SAC (Soft Actor-Critic) on the same dense Reach task. SAC is off-policy, meaning it reuses old data through a replay buffer. This is more sample-efficient but adds complexity (target networks, entropy tuning). By running SAC on dense Reach, we validate the off-policy machinery before adding HER for sparse rewards in Chapter 4.</p>"},{"location":"tutorials/ch02_ppo_dense_reach/#references","title":"References","text":"<ol> <li> <p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv:1707.06347.</p> </li> <li> <p>Schulman, J., Moritz, P., Levine, S., Jordan, M., &amp; Abbeel, P. (2015). High-Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv:1506.02438.</p> </li> <li> <p>Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., &amp; Meger, D. (2018). Deep Reinforcement Learning that Matters. AAAI.</p> </li> <li> <p>Stable Baselines3 Documentation: https://stable-baselines3.readthedocs.io/</p> </li> <li> <p>Spinning Up in Deep RL: https://spinningup.openai.com/</p> </li> </ol>"},{"location":"tutorials/ch03_sac_dense_reach/","title":"Chapter 3: SAC on Dense Reach -- Validating the Off-Policy Stack","text":""},{"location":"tutorials/ch03_sac_dense_reach/#what-this-chapter-is-really-about","title":"What This Chapter Is Really About","text":"<p>Chapter 2 validated our training pipeline with PPO. We achieved 100% success on dense Reach. The infrastructure works. But PPO has a fundamental limitation: it discards data after every update.</p> <p>The result: A SAC policy that learned to reach any point in 3D space, matching PPO's performance while building the off-policy machinery we need for HER.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#early-training-exploration-30k-steps-entropy-047","title":"Early Training: Exploration (30k steps, entropy = 0.47)","text":"<p>At 30k steps, the policy is still exploring. The high entropy coefficient (0.47) encourages random-ish actions. The gripper moves erratically, rarely reaching the target. Success rate: 0%.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#final-training-exploitation-1m-steps-entropy-00004","title":"Final Training: Exploitation (1M steps, entropy = 0.0004)","text":"<p>At 1M steps, the policy has converged. The entropy coefficient dropped to 0.0004 (nearly deterministic). The gripper moves directly to the target. Success rate: 100%.</p> <p>This is the entropy bonus in action: Early training explores broadly (high entropy), late training exploits what was learned (low entropy). SAC auto-tunes this tradeoff.</p> <p>PPO's data inefficiency is wasteful. Each transition took real simulation time to collect. PPO uses it for a few gradient steps, then throws it away. For dense-reward tasks where signal is plentiful, this inefficiency is tolerable. For sparse-reward tasks (coming in Chapter 4), it's catastrophic.</p> <p>Off-policy methods solve this by storing transitions in a replay buffer and reusing them across many updates. This is dramatically more sample-efficient. But it introduces new failure modes: stale data, value overestimation, training instability.</p> <p>This chapter validates the off-policy machinery on dense Reach--the same \"easy\" task where we know success is achievable. If SAC fails here, the bug is in our off-policy implementation, not the task difficulty.</p> <p>By the end, you will have: 1. A trained SAC policy matching or exceeding PPO's performance 2. Understanding of maximum-entropy RL and why it matters 3. Replay buffer diagnostics to detect common off-policy pathologies 4. Throughput benchmarks for scaling to larger experiments</p>"},{"location":"tutorials/ch03_sac_dense_reach/#part-0-setting-the-stage","title":"Part 0: Setting the Stage","text":""},{"location":"tutorials/ch03_sac_dense_reach/#01-why-sac-after-ppo","title":"0.1 Why SAC After PPO?","text":"<p>We're not choosing SAC because it's \"better\"--we're building toward HER (Hindsight Experience Replay), which requires off-policy learning. The dependency chain:</p> <pre><code>Dense Reach + PPO (Ch 2)      -- validates training loop\n        |\n        v\nDense Reach + SAC (Ch 3)      -- validates off-policy machinery  &lt;-- YOU ARE HERE\n        |\n        v\nSparse Reach + SAC + HER (Ch 4) -- validates goal relabeling\n        |\n        v\nHarder tasks (Ch 5+)          -- builds on validated infrastructure\n</code></pre> <p>Each step isolates one new component. If SAC fails on dense Reach, we know the bug is in SAC, not the environment. If HER fails on sparse Reach, we know the bug is in HER, not SAC.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#02-the-diagnostic-mindset-again","title":"0.2 The Diagnostic Mindset (Again)","text":"<p>Dense Reach with SAC should achieve similar performance to PPO--both are solving the same task with the same amount of signal. If SAC performs much worse: - Value overestimation (Q-values growing unbounded) - Entropy collapse (exploration stopped too early) - Replay buffer issues (stale data, wrong sampling)</p> <p>The diagnostics callback we'll use catches these early.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#part-1-why-maximum-entropy-reinforcement-learning","title":"Part 1: WHY -- Maximum Entropy Reinforcement Learning","text":""},{"location":"tutorials/ch03_sac_dense_reach/#11-the-standard-rl-objective-review","title":"1.1 The Standard RL Objective (Review)","text":"<p>Recall from Chapter 2 that standard RL maximizes expected return:</p> <pre><code>J(\\theta) = \\mathbb{E}\\left[ \\sum_{t=0}^{T} \\gamma^t R_t \\right]\n</code></pre> <p>This finds a policy that gets high reward. But there's a subtle problem: the optimal policy is deterministic. Once you know the best action for each state, why ever do anything else?</p>"},{"location":"tutorials/ch03_sac_dense_reach/#12-why-determinism-is-actually-a-problem","title":"1.2 Why Determinism Is Actually a Problem","text":"<p>In theory, a deterministic optimal policy is fine. In practice, it causes issues:</p> <p>Problem 1: Exploration Dies</p> <p>A deterministic policy exploits what it knows. If the current best action gets reward +10, the policy commits to it. But what if there's an action that would get +20, which the policy has never tried because it stopped exploring?</p> <p>Problem 2: Brittleness</p> <p>A policy that commits fully to one action per state is fragile. Small perturbations (observation noise, model mismatch) can push it into unfamiliar states where it has no idea what to do.</p> <p>Problem 3: Training Instability</p> <p>When the policy is nearly deterministic, small value estimate changes cause large behavioral changes (the \"winning\" action flips). This amplifies noise in the training process.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#13-the-maximum-entropy-objective","title":"1.3 The Maximum Entropy Objective","text":"<p>SAC modifies the objective to include an entropy bonus:</p> <pre><code>J(\\theta) = \\mathbb{E}\\left[ \\sum_{t=0}^{T} \\gamma^t \\left( R_t + \\alpha \\mathcal{H}(\\pi(\\cdot | s_t)) \\right) \\right]\n</code></pre> <p>where: - \\(\\mathcal{H}(\\pi(\\cdot | s))\\) = entropy of the policy at state \\(s\\), measuring how \"spread out\" the action distribution is - \\(\\alpha &gt; 0\\) = temperature parameter, controlling the exploration-exploitation tradeoff</p> <p>Definition (Entropy). For a continuous policy outputting a Gaussian distribution over actions, the entropy is:</p> <pre><code>\\mathcal{H}(\\pi(\\cdot | s)) = \\frac{1}{2} \\ln((2\\pi e)^d |\\Sigma|)\n</code></pre> <p>where \\(d\\) is the action dimension and \\(|\\Sigma|\\) is the determinant of the covariance matrix. Higher entropy means the policy is more random; lower entropy means more deterministic.</p> <p>The key insight: The optimal policy no longer commits fully to one action. Instead, it prefers actions proportionally to their Q-values:</p> <pre><code>\\pi^*(a | s) \\propto \\exp(Q^*(s, a) / \\alpha)\n</code></pre> <p>This is a Boltzmann distribution (or \"softmax\" over continuous actions). High-Q actions are more likely, but low-Q actions still have some probability. The temperature \\(\\alpha\\) controls the sharpness: - \\(\\alpha \\to 0\\): deterministic (argmax) - \\(\\alpha \\to \\infty\\): uniform random</p>"},{"location":"tutorials/ch03_sac_dense_reach/#14-automatic-temperature-tuning","title":"1.4 Automatic Temperature Tuning","text":"<p>Choosing \\(\\alpha\\) manually is tricky--too low and you lose exploration, too high and you never exploit. SAC can learn \\(\\alpha\\) automatically by targeting a desired entropy level:</p> <pre><code>\\alpha^* = \\arg\\min_\\alpha \\mathbb{E}_{a \\sim \\pi}\\left[ -\\alpha \\log \\pi(a|s) - \\alpha \\bar{\\mathcal{H}} \\right]\n</code></pre> <p>where \\(\\bar{\\mathcal{H}}\\) is the target entropy (typically set to \\(-\\dim(\\mathcal{A})\\), the negative action dimension).</p> <p>Translation: \"Adjust \\(\\alpha\\) so that the policy's entropy stays near \\(\\bar{\\mathcal{H}}\\).\"</p> <p>This means SAC adapts its exploration automatically. Early in training, when the policy is uncertain, entropy is high naturally. Late in training, when the policy is confident, \\(\\alpha\\) decreases to allow more exploitation.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#15-why-this-matters-for-robotics","title":"1.5 Why This Matters for Robotics","text":"<p>In robotics, brittleness kills. A policy that works perfectly in simulation but fails on real hardware is useless. Maximum entropy encourages:</p> <ol> <li>Diverse training data: The policy explores many actions, seeing more of the state space</li> <li>Robust behaviors: The policy doesn't commit fully to any single action, making it more tolerant to noise</li> <li>Smoother training: The Boltzmann-style policy changes gradually as Q-values change</li> </ol>"},{"location":"tutorials/ch03_sac_dense_reach/#part-2-how-the-sac-algorithm","title":"Part 2: HOW -- The SAC Algorithm","text":""},{"location":"tutorials/ch03_sac_dense_reach/#21-the-components","title":"2.1 The Components","text":"<p>SAC maintains five networks (in Stable Baselines 3's implementation):</p> Network Purpose Updates Actor \\(\\pi_\\theta\\) Maps states to action distributions Policy gradient Critic 1 \\(Q_{\\phi_1}\\) Estimates Q-values Bellman backup Critic 2 \\(Q_{\\phi_2}\\) Second Q estimate (reduces overestimation) Bellman backup Target Critic 1 \\(Q_{\\bar{\\phi}_1}\\) Stable target for critic updates Polyak averaging Target Critic 2 \\(Q_{\\bar{\\phi}_2}\\) Stable target for critic updates Polyak averaging <p>Why two critics? Q-learning tends to overestimate values because we take a max over noisy estimates. Using two critics and taking the minimum reduces this bias. This is called clipped double Q-learning.</p> <p>Why target networks? If we update the critic using its own predictions as targets, we get a moving target problem. Target networks are slow-moving copies that provide stable targets:</p> <pre><code>\\bar{\\phi} \\leftarrow \\tau \\phi + (1 - \\tau) \\bar{\\phi}\n</code></pre> <p>with \\(\\tau = 0.005\\) typically (slow updates).</p>"},{"location":"tutorials/ch03_sac_dense_reach/#22-the-training-loop","title":"2.2 The Training Loop","text":"<pre><code>repeat:\n    1. Collect transitions using current policy, store in replay buffer\n    2. Sample minibatch from replay buffer\n    3. Update critics using Bellman backup (target uses min of two Q-targets)\n    4. Update actor to maximize Q + entropy\n    5. Update temperature alpha (if auto-tuning)\n    6. Soft-update target networks\n</code></pre> <p>Step 1: Collect Data</p> <p>Unlike PPO, SAC collects data continuously. Each step: 1. Get action from policy (with sampling for exploration) 2. Execute in environment 3. Store \\((s, a, r, s', done)\\) in replay buffer</p> <p>The replay buffer is circular--when full, old transitions are overwritten.</p> <p>Step 2: Sample Minibatch</p> <p>Uniformly sample <code>batch_size</code> transitions from the buffer. This is where sample reuse happens--the same transition might be sampled many times across training.</p> <p>Step 3: Update Critics</p> <p>For each critic, minimize the Bellman error:</p> <pre><code>L(\\phi_i) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{B}}\\left[ \\left( Q_{\\phi_i}(s, a) - y \\right)^2 \\right]\n</code></pre> <p>where the target \\(y\\) is:</p> <pre><code>y = r + \\gamma (1 - done) \\left[ \\min_{j=1,2} Q_{\\bar{\\phi}_j}(s', a') - \\alpha \\log \\pi(a' | s') \\right]\n</code></pre> <p>with \\(a' \\sim \\pi(\\cdot | s')\\).</p> <p>Translation: \"The Q-value of \\((s, a)\\) should equal the reward plus the (discounted, entropy-adjusted) value of the next state.\"</p> <p>Step 4: Update Actor</p> <p>Maximize expected Q-value plus entropy:</p> <pre><code>L(\\theta) = \\mathbb{E}_{s \\sim \\mathcal{B}, a \\sim \\pi_\\theta}\\left[ \\alpha \\log \\pi_\\theta(a | s) - \\min_{i=1,2} Q_{\\phi_i}(s, a) \\right]\n</code></pre> <p>Translation: \"Choose actions that have high Q-values while maintaining high entropy.\"</p>"},{"location":"tutorials/ch03_sac_dense_reach/#23-key-differences-from-ppo","title":"2.3 Key Differences from PPO","text":"Aspect PPO SAC Data reuse None (on-policy) Extensive (replay buffer) Exploration Via entropy bonus (optional) Via entropy bonus (core) Sample efficiency Low High Stability High (clipped updates) Medium (moving targets) Complexity Lower Higher (more networks)"},{"location":"tutorials/ch03_sac_dense_reach/#24-key-hyperparameters","title":"2.4 Key Hyperparameters","text":"Parameter Default What It Controls <code>buffer_size</code> 1,000,000 Replay buffer capacity <code>batch_size</code> 256 Minibatch size for updates <code>learning_starts</code> 10,000 Steps before training begins <code>tau</code> 0.005 Target network update rate <code>ent_coef</code> \"auto\" Entropy temperature (auto-tuned) <code>learning_rate</code> 3e-4 Gradient step size"},{"location":"tutorials/ch03_sac_dense_reach/#part-3-what-running-the-experiment","title":"Part 3: WHAT -- Running the Experiment","text":""},{"location":"tutorials/ch03_sac_dense_reach/#31-the-one-command-version","title":"3.1 The One-Command Version","text":"<pre><code>bash docker/dev.sh python scripts/ch03_sac_dense_reach.py all --seed 0\n</code></pre> <p>This runs training (with diagnostics), evaluation, and comparison to PPO.</p> <p>For faster iteration (~2 minutes):</p> <pre><code>bash docker/dev.sh python scripts/ch03_sac_dense_reach.py train --total-steps 100000\n</code></pre>"},{"location":"tutorials/ch03_sac_dense_reach/#32-what-to-expect","title":"3.2 What to Expect","text":"<p>Training progress milestones for SAC (note: <code>learning_starts=10000</code> means no training for first 10k steps):</p> Timesteps Success Rate What's Happening 0-10k 0% Collecting random data (no training yet) 10k-50k 0-5% Training begins, Q-values learning 50k-150k 20-60% Rapid improvement, entropy decreasing 150k-400k 80-99% Policy converging 400k-1M 100% Fine-tuning, entropy near zero <p>Our test run achieved: - 100% success rate after ~300k steps - 18.6mm average goal distance (well within 50mm threshold) - ~594 steps/second throughput on NVIDIA GB10 - Entropy coefficient: dropped from 0.47 to 0.0004 (policy became deterministic)</p>"},{"location":"tutorials/ch03_sac_dense_reach/#32-what-the-diagnostics-callback-logs","title":"3.2 What the Diagnostics Callback Logs","text":"<p>Our custom <code>SACDiagnosticsCallback</code> logs to TensorBoard:</p> Metric What It Means Healthy Range <code>replay/q1_mean</code>, <code>q2_mean</code> Average Q-values Should stabilize, not explode <code>replay/q_min_mean</code> Min of two Q-values Tracks actual value estimates <code>replay/ent_coef</code> Temperature \\(\\alpha\\) Should decrease over training <code>replay/reward_mean</code> Average reward in buffer Task-dependent <code>replay/goal_distance_mean</code> How far from goals Should decrease <code>replay/goal_within_threshold</code> Fraction within success Should increase"},{"location":"tutorials/ch03_sac_dense_reach/#33-what-to-watch-for","title":"3.3 What to Watch For","text":"<p>Q-Value Behavior: - Healthy: Q-values stabilize in a reasonable range (we observed <code>q_min_mean ~ -1.5</code>) - Unhealthy: Q-values grow unbounded (&gt;100, continuing to increase)</p> <p>Our run showed Q-values starting positive (~18 at 30k steps during random exploration) then settling to ~-1.5 as the policy learned. This is healthy--the critic learned accurate value estimates.</p> <p>Entropy Coefficient: - Healthy: Starts high, gradually decreases (we observed: 0.47 -&gt; 0.0004) - Unhealthy: Drops to near-zero in first 10k steps (exploration collapsed)</p> <p>Our run showed gradual decrease over 1M steps, indicating the auto-tuning worked correctly. The final value (0.0004) means the policy became nearly deterministic--appropriate for a solved task.</p> <p>Goal Distance (in replay buffer): - Note: This shows the average distance across the entire buffer, not current policy performance - Our run showed ~0.20m mean distance (historical average including early random data) - Current policy performance is measured by <code>rollout/success_rate</code> and evaluation metrics</p>"},{"location":"tutorials/ch03_sac_dense_reach/#34-throughput-scaling","title":"3.4 Throughput Scaling","text":"<p>Off-policy methods can benefit from parallel environments, but the relationship is complex (more envs = more data, but also more stale):</p> <pre><code>bash docker/dev.sh python scripts/ch03_sac_dense_reach.py throughput --n-envs-list 1,2,4,8,16\n</code></pre> <p>This measures steps/second for different <code>n_envs</code> values. Expect: - Near-linear scaling up to some point - Diminishing returns as CPU becomes the bottleneck - Sweet spot typically around 4-16 envs for SAC</p>"},{"location":"tutorials/ch03_sac_dense_reach/#35-actual-results","title":"3.5 Actual Results","text":"<p>On FetchReachDense-v4 with 1M steps (NVIDIA GB10):</p> Metric PPO (Ch 2) SAC (Actual) Success Rate 100% 100% Mean Return -0.40 -1.06 Final Distance 4.6mm 18.6mm Action Smoothness 1.40 1.68 Training Time ~6 min ~28 min Throughput ~1300 fps ~594 fps <p>Analysis: - Both achieve 100% success--the off-policy stack is validated - SAC has higher final distance (18.6mm vs 4.6mm) but still well within the 50mm success threshold - SAC is slower due to more network updates (actor + 2 critics + 2 targets) per step - SAC's slightly less smooth actions reflect the entropy bonus encouraging exploration</p>"},{"location":"tutorials/ch03_sac_dense_reach/#36-understanding-gpu-utilization","title":"3.6 Understanding GPU Utilization","text":"<p>You may notice low GPU utilization (~5-10%) during training:</p> <pre><code>$ nvidia-smi dmon -s u\n# gpu     sm    mem\n    0      7      0\n</code></pre> <p>This is expected for RL training. The bottleneck is CPU, not GPU:</p> Component Runs On Time Fraction MuJoCo physics simulation CPU ~60-70% Environment step/reset CPU ~10-15% Neural network forward GPU ~5-10% Neural network backward GPU ~10-15% Replay buffer operations CPU ~5% <p>With small batch sizes (256) and simple MLPs, GPU operations complete in microseconds. The GPU idles while waiting for CPU-bound simulation. This is why throughput (~600 fps) is limited by CPU, not GPU.</p> <p>To increase GPU utilization: - Larger batch sizes (but diminishing returns for small networks) - Multiple parallel training workers (adds complexity) - Vision-based policies with CNNs (Week 8+)</p> <p>For our curriculum, ~600 fps is sufficient. Don't optimize prematurely.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#37-generating-demo-videos","title":"3.7 Generating Demo Videos","text":"<p>To visualize your trained policy, use the video generation script:</p> <pre><code># Generate videos from SAC checkpoint\nbash docker/dev.sh python scripts/generate_demo_videos.py \\\n    --ckpt checkpoints/sac_FetchReachDense-v4_seed0.zip \\\n    --env FetchReachDense-v4 \\\n    --n-episodes 5 \\\n    --out videos/sac_reach_demo \\\n    --grid \\\n    --gif\n</code></pre> <p>This creates: - <code>videos/sac_reach_demo.mp4</code> - Compilation of all episodes - <code>videos/sac_reach_demo_grid.mp4</code> - 2x2 grid showing 4 episodes simultaneously - <code>videos/sac_reach_demo_grid.gif</code> - GIF version for documentation/social media</p> <p>Video annotations: Each frame shows: - Distance to target - Real-time distance in centimeters - \"TARGET REACHED!\" - Green text when within 5cm threshold - Legend - Red dot = target position, Green dot = gripper position</p> <p>The script automatically: 1. Loads the trained model (works with PPO, SAC, or TD3) 2. Enlarges the target sphere for visibility 3. Adds text overlays with goal distance 4. Creates both MP4 and GIF formats</p>"},{"location":"tutorials/ch03_sac_dense_reach/#part-4-understanding-what-you-built","title":"Part 4: Understanding What You Built","text":""},{"location":"tutorials/ch03_sac_dense_reach/#41-the-replay-buffer","title":"4.1 The Replay Buffer","text":"<p>The replay buffer is a circular array storing transitions:</p> <pre><code>[t_0, t_1, t_2, ..., t_{n-1}, t_n, t_{n+1}, ...]\n ^-- oldest                          ^-- newest\n</code></pre> <p>When full, new transitions overwrite the oldest. This means: - Recent experience is always available - Very old experience is eventually forgotten - Buffer size controls the \"memory horizon\"</p> <p>For 1M buffer and 1M training steps, every transition is seen roughly once on average. Increase buffer size for longer memory; decrease for faster adaptation.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#42-the-min-q-trick","title":"4.2 The Min-Q Trick","text":"<p>When computing the target for critic updates, SAC uses:</p> <pre><code>\\min_{j=1,2} Q_{\\bar{\\phi}_j}(s', a')\n</code></pre> <p>This seems pessimistic--why take the minimum? Because Q-learning systematically overestimates:</p> <pre><code>\\mathbb{E}[\\max(Q_1, Q_2)] \\geq \\max(\\mathbb{E}[Q_1], \\mathbb{E}[Q_2])\n</code></pre> <p>Taking the min counteracts this bias, leading to more stable training.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#43-the-squashed-gaussian","title":"4.3 The Squashed Gaussian","text":"<p>SAC uses a \"squashed Gaussian\" for the policy: 1. Sample from a Gaussian: \\(z \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) 2. Squash through tanh: \\(a = \\tanh(z)\\)</p> <p>This ensures actions are bounded (important for robotics with joint limits). The log probability calculation accounts for the squashing via a Jacobian correction.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#part-5-exercises","title":"Part 5: Exercises","text":""},{"location":"tutorials/ch03_sac_dense_reach/#exercise-31-reproduce-the-baseline","title":"Exercise 3.1: Reproduce the Baseline","text":"<p>Run SAC on FetchReachDense-v4 and verify &gt;95% success rate:</p> <pre><code>bash docker/dev.sh python scripts/ch03_sac_dense_reach.py all --seed 0\n</code></pre>"},{"location":"tutorials/ch03_sac_dense_reach/#exercise-32-compare-learning-curves","title":"Exercise 3.2: Compare Learning Curves","text":"<p>Open TensorBoard and compare PPO vs SAC:</p> <pre><code>bash docker/dev.sh tensorboard --logdir runs --bind_all\n</code></pre> <p>Questions to answer: - Which reaches high success faster? - How do the value loss curves differ? - What happens to SAC's entropy coefficient over training?</p>"},{"location":"tutorials/ch03_sac_dense_reach/#exercise-33-throughput-analysis","title":"Exercise 3.3: Throughput Analysis","text":"<p>Run the throughput experiment:</p> <pre><code>bash docker/dev.sh python scripts/ch03_sac_dense_reach.py throughput\n</code></pre> <p>Plot steps/sec vs n_envs. Where does scaling stop being linear?</p>"},{"location":"tutorials/ch03_sac_dense_reach/#exercise-34-q-value-analysis-written","title":"Exercise 3.4: Q-Value Analysis (Written)","text":"<p>Watch the Q-value metrics in TensorBoard. Answer: 1. Do Q1 and Q2 diverge significantly, or stay close? 2. What is the typical range of Q-values for this task? 3. How would you detect Q-value overestimation if it occurred?</p>"},{"location":"tutorials/ch03_sac_dense_reach/#exercise-35-temperature-ablation","title":"Exercise 3.5: Temperature Ablation","text":"<p>Train with fixed entropy coefficients instead of auto-tuning:</p> <pre><code># In train.py, modify ent_coef to a fixed value:\n# ent_coef=0.1 (high exploration)\n# ent_coef=0.01 (medium)\n# ent_coef=0.001 (low exploration)\n</code></pre> <p>How does fixed vs auto-tuned entropy affect: - Final performance? - Training stability? - Time to convergence?</p>"},{"location":"tutorials/ch03_sac_dense_reach/#part-6-common-failures-and-solutions","title":"Part 6: Common Failures and Solutions","text":""},{"location":"tutorials/ch03_sac_dense_reach/#q-values-explode-1000","title":"\"Q-values explode (&gt;1000)\"","text":"<p>Symptom: <code>replay/q_min_mean</code> keeps growing without bound.</p> <p>Cause: Overestimation feedback loop. Q-targets use overestimated Q-values, which train the critic to overestimate further.</p> <p>Solutions: 1. Verify rewards are bounded (FetchReachDense: [-1, 0]) 2. Check discount factor (should be &lt;1, typically 0.99) 3. Reduce learning rate 4. Verify target networks are updating (tau should be small)</p>"},{"location":"tutorials/ch03_sac_dense_reach/#entropy-coefficient-goes-to-zero-immediately","title":"\"Entropy coefficient goes to zero immediately\"","text":"<p>Symptom: <code>replay/ent_coef</code> drops to &lt;0.01 within first 10k steps.</p> <p>Cause: Target entropy too low, or policy collapsed to near-deterministic.</p> <p>Solutions: 1. Increase target entropy (default is -dim(action)) 2. Check action space bounds (if very wide, entropy scale changes) 3. Use fixed entropy coefficient initially to debug</p>"},{"location":"tutorials/ch03_sac_dense_reach/#success-rate-stalls-below-50","title":"\"Success rate stalls below 50%\"","text":"<p>Symptom: Learning plateaus well below PPO baseline.</p> <p>Cause: Often insufficient exploration or replay buffer issues.</p> <p>Solutions: 1. Check entropy coefficient isn't too low 2. Verify <code>learning_starts</code> isn't too high (not using early data) 3. Check batch size (too small = high variance, too large = slow updates)</p>"},{"location":"tutorials/ch03_sac_dense_reach/#training-is-much-slower-than-ppo","title":"\"Training is much slower than PPO\"","text":"<p>Expected: SAC is slower per-step (more networks to update).</p> <p>Unexpected slowness causes: 1. Buffer operations (should be O(1) for sampling) 2. GPU memory (buffer can be large) 3. Too many gradient steps per env step</p>"},{"location":"tutorials/ch03_sac_dense_reach/#part-7-looking-ahead","title":"Part 7: Looking Ahead","text":"<p>With SAC validated on dense Reach, we've confirmed: 1. Replay buffer correctly stores and samples transitions 2. Dual critics work without divergence 3. Entropy tuning behaves as expected 4. Off-policy learning achieves good performance</p> <p>Chapter 4 introduces HER (Hindsight Experience Replay) for sparse rewards. The key insight: a failed trajectory to goal \\(g\\) is a successful trajectory to whatever goal we actually reached. HER relabels transitions to manufacture success signal from failure.</p> <p>This requires off-policy learning (SAC or TD3)--on-policy methods like PPO cannot use HER because they can't reuse relabeled data. Our validated SAC stack is the foundation.</p>"},{"location":"tutorials/ch03_sac_dense_reach/#references","title":"References","text":"<ol> <li> <p>Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. ICML.</p> </li> <li> <p>Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., &amp; Levine, S. (2018). Soft Actor-Critic Algorithms and Applications. arXiv:1812.05905.</p> </li> <li> <p>Fujimoto, S., van Hoof, H., &amp; Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. ICML. (TD3 paper, introduces clipped double-Q)</p> </li> <li> <p>Stable Baselines3 SAC Documentation: https://stable-baselines3.readthedocs.io/en/master/modules/sac.html</p> </li> </ol>"}]}